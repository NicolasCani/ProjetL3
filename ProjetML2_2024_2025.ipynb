{"cells":[{"cell_type":"markdown","metadata":{"id":"rEfSujULiOkb"},"source":["<H1> Les jeux de données pour le projet </H1>\n","\n","Dans ce notebook nous présentons les jeux de données utilisés pour le projet. Nous proposons également des fonctions pour permettre de pouvoir facilement les données.   \n","\n","Il n'y a donc plus qu'à chercher les meilleurs modèles et à répondre aux questions de l'énoncé du projet.   \n","\n","Bon courage !\n","\n","ps : il y a trois jeux de données et ils sont très différents donc attention vous aurez peut être 3 modèles différents."]},{"cell_type":"markdown","metadata":{"id":"_VQSC8frX5Ao"},"source":["## Installation\n"]},{"cell_type":"markdown","metadata":{"id":"5zICeCUqYCfS"},"source":["\n","Avant de commencer, il est nécessaire de déjà posséder dans son environnement toutes les librairies utiles. Dans la seconde cellule nous importons toutes les librairies qui seront utiles à ce notebook. Il se peut que, lorsque vous lanciez l'éxecution de cette cellule, une soit absente. Dans ce cas il est nécessaire de l'installer. Pour cela dans la cellule suivante utiliser la commande :  \n","\n","*! pip install nom_librairie*  \n","\n","**Attention :** il est fortement conseillé lorsque l'une des librairies doit être installer de relancer le kernel de votre notebook.\n","\n","**Remarque :** même si toutes les librairies sont importées dès le début, les librairies utiles pour des fonctions présentées au cours de ce notebook sont ré-importées de manière à indiquer d'où elles viennent et ainsi faciliter la réutilisation de la fonction dans un autre projet.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"I0OpnDBHYHmv"},"outputs":[],"source":["# utiliser cette cellule pour installer les librairies manquantes\n","# pour cela il suffit de taper dans cette cellule : !pip install nom_librairie_manquante\n","# d'exécuter la cellule et de relancer la cellule suivante pour voir si tout se passe bien\n","# recommencer tant que toutes les librairies ne sont pas installées ...\n","\n","# sous Colab il faut déjà intégrer ces deux librairies\n","\n","#!pip install umap-learn[plot]\n","#!pip install holoviews\n","#!pip install -U ipykernel\n","\n","# eventuellement ne pas oublier de relancer le kernel du notebook\n","!pip install --upgrade tensorflow\n","!pip install optuna\n","!pip install opencv-python-headless"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FZw2C6r8YJWW"},"outputs":[],"source":["# Importation des différentes librairies utiles pour le notebook\n","\n","#Sickit learn met régulièrement à jour des versions et\n","#indique des futurs warnings.\n","#ces deux lignes permettent de ne pas les afficher.\n","import warnings\n","warnings.filterwarnings(\"ignore\", category=FutureWarning)\n","\n","# librairies générales\n","import pickle\n","import pandas as pd\n","from scipy.stats import randint\n","import numpy as np\n","import string\n","import time\n","import base64\n","import re\n","import sys\n","import copy\n","import random\n","from numpy import mean\n","from numpy import std\n","\n","\n","# librairie affichage\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from PIL import Image\n","import plotly.graph_objs as go\n","import plotly.offline as py\n","\n","from sklearn.metrics import confusion_matrix\n","from sklearn.manifold import TSNE\n","from sklearn.decomposition import PCA\n","from sklearn.model_selection import KFold\n","from sklearn.model_selection import cross_val_score\n","from sklearn.pipeline import Pipeline\n","from sklearn.naive_bayes import GaussianNB\n","from sklearn.metrics import accuracy_score\n","\n","# TensorFlow et keras\n","import tensorflow as tf\n","from keras import layers\n","from keras import models\n","from keras import optimizers\n","import tensorflow as tf\n","from keras import Input\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","#from keras.preprocessing.image import ImageDataGenerator\n","from keras.preprocessing.image import img_to_array, load_img\n","from keras.callbacks import ModelCheckpoint, EarlyStopping\n","import keras\n","from keras.models import Sequential\n","from keras.layers import Dense, Dropout, Flatten\n","from keras.layers import Conv2D, MaxPooling2D\n","from keras.preprocessing import image\n","from tqdm import tqdm\n","from keras.models import load_model\n","from sklearn.model_selection import KFold\n","from keras.datasets import fashion_mnist\n","from tensorflow.keras.utils import to_categorical\n","from keras.models import Sequential\n","from keras.layers import Conv2D\n","from keras.layers import MaxPooling2D\n","from keras.layers import Dense\n","from keras.layers import Flatten\n","from tensorflow.keras.optimizers import SGD\n","from tensorflow.keras.optimizers import Adam\n","import os\n","from os import listdir\n","from os.path import isfile, join\n","import cv2\n","import glob\n","import csv\n","from statistics import mean, stdev\n","import optuna\n","from sklearn.metrics import classification_report\n","import tensorflow.keras.backend as K\n","from gc import collect\n","from keras.layers import Lambda\n","from tensorflow.keras.applications.resnet50 import ResNet50\n","from tensorflow.keras.applications.densenet import DenseNet121"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5n70qVaRWwII"},"outputs":[],"source":["def plot_curves(histories):\n","    \"\"\"\n","    Fonction pour afficher les courbes de loss et d'accuracy moyennées et écart-types à travers les k-folds.\n","\n","    Paramètres :\n","    - histories (list) : Historique d'entraînement des différents plis K-folds.\n","    \"\"\"\n","\n","    # Initialisation des figures\n","    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\n","\n","    # Extraction du nombre d'époques d'après l'un des historiques\n","    epochs = range(len(histories[0].history['loss']))\n","\n","    # Calcul des moyennes et des écart-types pour chaque époque\n","    mean_loss = np.mean([history.history['loss'] for history in histories], axis=0)\n","    std_loss = np.std([history.history['loss'] for history in histories], axis=0)\n","\n","    mean_val_loss = np.mean([history.history['val_loss'] for history in histories], axis=0)\n","    std_val_loss = np.std([history.history['val_loss'] for history in histories], axis=0)\n","\n","    mean_accuracy = np.mean([history.history['accuracy'] for history in histories], axis=0)\n","    std_accuracy = np.std([history.history['accuracy'] for history in histories], axis=0)\n","\n","    mean_val_accuracy = np.mean([history.history['val_accuracy'] for history in histories], axis=0)\n","    std_val_accuracy = np.std([history.history['val_accuracy'] for history in histories], axis=0)\n","\n","    # Couleurs pour les courbes\n","    train_color = 'blue'\n","    val_color = 'orange'\n","\n","    # Courbes de loss avec moyenne et écart-type\n","    ax1.plot(epochs, mean_loss, color=train_color, label='Train')\n","    ax1.fill_between(epochs, mean_loss - std_loss, mean_loss + std_loss, color=train_color, alpha=0.2)\n","\n","    ax1.plot(epochs, mean_val_loss, color=val_color, label='Validation')\n","    ax1.fill_between(epochs, mean_val_loss - std_val_loss, mean_val_loss + std_val_loss, color=val_color, alpha=0.2)\n","\n","    # Courbes d'accuracy avec moyenne et écart-type\n","    ax2.plot(epochs, mean_accuracy, color=train_color, label='Train')\n","    ax2.fill_between(epochs, mean_accuracy - std_accuracy, mean_accuracy + std_accuracy, color=train_color, alpha=0.2)\n","\n","    ax2.plot(epochs, mean_val_accuracy, color=val_color, label='Validation')\n","    ax2.fill_between(epochs, mean_val_accuracy - std_val_accuracy, mean_val_accuracy + std_val_accuracy, color=val_color, alpha=0.2)\n","\n","    # Titres, labels et légendes\n","    ax1.set_title(f'Loss (k={len(histories)})')\n","    ax1.set_xlabel('Epoch')\n","    ax1.set_ylabel('Loss')\n","    ax1.legend()\n","\n","    ax2.set_title(f'Accuracy (k={len(histories)})')\n","    ax2.set_xlabel('Epoch')\n","    ax2.set_ylabel('Accuracy')\n","    ax2.legend()\n","\n","    plt.show()\n","\n","def plot_loss_accuracy(history):\n","    \"\"\"\n","    Fonction pour afficher les courbes de loss et d'accuracy pour une seule exécution.\n","\n","    Paramètres :\n","    - history (History) : Historique d'entraînement retourné par model.fit().\n","    \"\"\"\n","\n","    # Extraction des données d'historique\n","    epochs = range(len(history.history['loss']))\n","\n","    # Calcul des moyennes pour chaque époque\n","    loss = history.history['loss']\n","    val_loss = history.history['val_loss']\n","    accuracy = history.history['accuracy']\n","    val_accuracy = history.history['val_accuracy']\n","\n","    # Initialisation des figures\n","    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\n","\n","    # Courbes de loss\n","    ax1.plot(epochs, loss, color='blue', label='Train Loss')\n","    ax1.plot(epochs, val_loss, color='orange', label='Validation Loss')\n","    ax1.set_title('Loss')\n","    ax1.set_xlabel('Epochs')\n","    ax1.set_ylabel('Loss')\n","    ax1.legend()\n","\n","    # Courbes d'accuracy\n","    ax2.plot(epochs, accuracy, color='blue', label='Train Accuracy')\n","    ax2.plot(epochs, val_accuracy, color='orange', label='Validation Accuracy')\n","    ax2.set_title('Accuracy')\n","    ax2.set_xlabel('Epochs')\n","    ax2.set_ylabel('Accuracy')\n","    ax2.legend()\n","\n","    # Affichage des courbes\n","    plt.tight_layout()\n","    plt.show()\n","\n","def plot_confusion_matrix(cm, classes, title='Matrice de confusion', cmap=plt.cm.Blues):\n","    \"\"\"\n","    Affiche la matrice de confusion.\n","\n","    Parameters:\n","    - cm (array-like): Matrice de confusion (2D numpy array).\n","    - classes (list of str): Liste des noms des classes correspondant aux dimensions de la matrice.\n","    - title (str): Titre du graphique (par défaut 'Matrice de confusion').\n","    - cmap (matplotlib.colors.Colormap): Carte des couleurs à utiliser pour le graphique (par défaut plt.cm.Blues).\n","\t\"\"\"\n","    plt.figure(figsize=(8, 6))\n","    sns.heatmap(cm, annot=True, fmt='d', cmap=cmap, xticklabels=classes, yticklabels=classes)\n","    plt.title(title)\n","    plt.ylabel('Vérité terrain')\n","    plt.xlabel('Prédictions')\n","    plt.tight_layout()\n","    plt.show()"]},{"cell_type":"markdown","metadata":{"id":"0s2sF7a-am4I"},"source":["Pour pouvoir sauvegarder sur votre répertoire Google Drive, il est nécessaire de fournir une autorisation. Pour cela il suffit d'éxecuter la ligne suivante et de saisir le code donné par Google."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"q1zULJ9daqP6"},"outputs":[],"source":["# pour monter son drive Google Drive local\n","from google.colab import drive\n","drive.mount('/content/gdrive',force_remount=True)"]},{"cell_type":"markdown","metadata":{"id":"0qe4Xtohau6v"},"source":["Corriger éventuellement la ligne ci-dessous pour mettre le chemin vers un répertoire spécifique dans votre répertoire Google Drive :"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ahDl5RV9tVfp"},"outputs":[],"source":["import sys\n","my_local_drive='/content/gdrive/MyDrive/Colab Notebooks/ML_FDS'\n","# Ajout du path pour les librairies, fonctions et données\n","sys.path.append(my_local_drive)\n","# Se positionner sur le répertoire associé\n","%cd $my_local_drive\n","\n","%pwd"]},{"cell_type":"markdown","metadata":{"id":"KlDbIEQR-DPw"},"source":["####Les jeux de données\n"]},{"cell_type":"markdown","metadata":{"id":"25Ci4vc2mz0i"},"source":["Récupération des jeux de données :      "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LTUWPaTYm4Tr"},"outputs":[],"source":["!wget https://www.lirmm.fr/~poncelet/Ressources/Tiger-Fox-Elephant.zip"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"imhdFg9uoIbs"},"outputs":[],"source":["import zipfile\n","print(\"Répertoire actuel :\", os.getcwd())\n","with zipfile.ZipFile(\"Tiger-Fox-Elephant.zip\",\"r\") as zip_ref:\n","    zip_ref.extractall(\"/content/gdrive/MyDrive/Colab Notebooks/ML_FDS/Data_Project\")"]},{"cell_type":"markdown","metadata":{"id":"iqC8-3wMZC1t"},"source":["\n","Il y a trois jeux de données différents : des tigres, des éléphants et des renards. Pour chacun d'entre eux il y a un ensemble d'images positive et un ensemble d'images négatives. Par exemple dans le répertoire *tiger* il n'y a que des images de tigre et dans le répertoire *Tiger_negative_class* il n'y a que des images d'animaux qui ne correspondent pas à des tigres.   \n","\n","Le code ci-dessous permet de visualiser quelques images contenues dans le répertoire *tiger*."]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"lVs5P1VmaAYr"},"outputs":[],"source":["mypath='Data_Project/Tiger-Fox-Elephant/tiger'\n","onlyfiles = [ f for f in listdir(mypath) if isfile(join(mypath,f)) ]\n","images = np.empty(len(onlyfiles), dtype=object)\n","for n in range(0, len(onlyfiles)):\n","  images[n] = cv2.imread( join(mypath,onlyfiles[n]) )\n","\n","\n","COLUMNS = 25 # Nombre d'images à afficher\n","\n","plt.figure(figsize=(15,15))\n","for i in range(COLUMNS):\n","    plt.subplot(5,5,i+1)\n","    plt.xticks([])\n","    plt.yticks([])\n","    plt.grid(False)\n","    # cv2 lit met les images en BGR et matplotlib lit du RGB\n","    # il faut donc convertir pour afficher les bonnes couleurs\n","    images[i] = cv2.cvtColor(images[i], cv2.COLOR_BGR2RGB)\n","    plt.imshow(images[i],cmap=plt.cm.binary)\n","    plt.xlabel('taille ' + str(images[i].shape))"]},{"cell_type":"markdown","metadata":{"id":"0eRP1h1fawL8"},"source":["Nous pouvons constater que les images ne sont pas de la même taille. Il faut donc les convertir. Une manière simple de faire et de faire la conversion lors de la lecture des images : ici nous convertissons toutes les images en 124x124."]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"SkR8CZEObaKx"},"outputs":[],"source":["IMG_SIZE=128\n","mypath='Data_Project/Tiger-Fox-Elephant/tiger'\n","onlyfiles = [ f for f in listdir(mypath) if isfile(join(mypath,f)) ]\n","images = np.empty(len(onlyfiles), dtype=object)\n","for n in range(0, len(onlyfiles)):\n","  images[n] = cv2.imread( join(mypath,onlyfiles[n]) )\n","  images[n]  = cv2.resize(images[n], (IMG_SIZE, IMG_SIZE))\n","\n","plt.figure(figsize=(15,15))\n","for i in range(COLUMNS):\n","    plt.subplot(5,5,i+1)\n","    plt.xticks([])\n","    plt.yticks([])\n","    plt.grid(False)\n","    # cv2 lit met les images en BGR et matplotlib lit du RGB\n","    # il faut donc convertir pour afficher les bonnes couleurs\n","    images[i] = cv2.cvtColor(images[i], cv2.COLOR_BGR2RGB)\n","    plt.imshow(images[i],cmap=plt.cm.binary)\n","    plt.xlabel('taille ' + str(images[i].shape))"]},{"cell_type":"markdown","metadata":{"id":"Ix_SIuG9b9fn"},"source":["**Créer le jeu de données**   \n","\n","Actuellement pour chaque animal nous avons un répertoire qui contient des images positives et un répertoire qui contient des images négatives. Pour pouvoir créer un jeu de données nous devons obtenir X et y. Les fonctions ci-dessous permettent de générer, à partir des répertoires, un jeu de données aléatoire pour X et y."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"n02QZLNadp9A"},"outputs":[],"source":["def create_training_data(path_data, list_classes):\n","  training_data=[]\n","  for classes in list_classes:\n","      path=os.path.join(path_data, classes)\n","      class_num=list_classes.index(classes)\n","      for img in os.listdir(path):\n","        try:\n","          img_array = cv2.imread(os.path.join(path,img),cv2.IMREAD_UNCHANGED)\n","          new_array = cv2.resize(img_array, (IMG_SIZE, IMG_SIZE))\n","          training_data.append([new_array, class_num])\n","        except Exception as e:\n","          pass\n","  return training_data\n","\n","def create_X_y (path_data, list_classes):\n","      # récupération des données\n","      training_data=create_training_data(path_data, list_classes)\n","      # tri des données\n","      #random.shuffle(training_data)\n","      # création de X et y\n","      X=[]\n","      y=[]\n","      for features, label in training_data:\n","        X.append(features)\n","        y.append(label)\n","      X=np.array(X).reshape(-1,IMG_SIZE, IMG_SIZE, 3)\n","      y=np.array(y)\n","      return X,y\n","\n","def plot_examples(X,y):\n","  plt.figure(figsize=(15,15))\n","  for i in range(COLUMNS):\n","    plt.subplot(5,5,i+1)\n","    plt.xticks([])\n","    plt.yticks([])\n","    plt.grid(False)\n","    # cv2 lit met les images en BGR et matplotlib lit du RGB\n","    X[i] = cv2.cvtColor(X[i], cv2.COLOR_BGR2RGB)\n","    plt.imshow(X[i]/255.,cmap=plt.cm.binary)\n","    plt.xlabel('classe ' + str(y[i]))"]},{"cell_type":"markdown","metadata":{"id":"smgGc_PbeDt_"},"source":["Définition de constante globale      \n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rK1qXTdyeIEF"},"outputs":[],"source":["# constantes globales\n","\n","IMG_SIZE=128\n","COLUMNS = 25 # Nombre d'images à afficher"]},{"cell_type":"markdown","metadata":{"id":"wmo8-X4VeyyL"},"source":["Pour les tigres :"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"GgIHX8KWemlj"},"outputs":[],"source":["my_path=\"Data_Project/Tiger-Fox-Elephant/\"\n","my_classes=['tiger','Tiger_negative_class']\n","X,y=create_X_y (my_path,my_classes)\n","print (\"Nombre de données : \",X.shape[0])\n","print (\"Taille d'une image pour connaître l'input du réseau\", X[0].shape)\n","#print (\"Distribution des labels dans le jeu d'apprentissage\")\n","#sns.countplot(np.array(y))\n","#plt.title(\"Nombre d'éléments par classe\")\n","# affichage\n","plot_examples(X,y)\n","\n","# Surtout ne pas oublier de normaliser les données avec :\n","X=X.astype('float')\n","X=X/255.0"]},{"cell_type":"markdown","metadata":{"id":"2daZYyk9gYAM"},"source":["Pour les éléphants :     "]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"KZIV7xlzgbYq"},"outputs":[],"source":["my_path=\"Data_Project/Tiger-Fox-Elephant/\"\n","my_classes=['elephant','Elephant_negative_class']\n","X,y=create_X_y (my_path,my_classes)\n","print (\"Nombre de données : \",X.shape[0])\n","print (\"Taille d'une image pour connaître l'input du réseau\", X[0].shape)\n","#print (\"Distribution des labels dans le jeu d'apprentissage\")\n","#sns.countplot(np.array(y))\n","#plt.title(\"Nombre d'éléments par classe\")\n","# affichage\n","plot_examples(X,y)\n","\n","# Surtout ne pas oublier de normaliser les données avec :\n","X=X.astype('float')\n","X=X/255.0"]},{"cell_type":"markdown","metadata":{"id":"v2bIEGTWgt8q"},"source":["Pour les renards :     \n"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"COgu0tUWgxUL"},"outputs":[],"source":["my_path=\"Data_Project/Tiger-Fox-Elephant/\"\n","my_classes=['fox','Fox_negative_class']\n","X,y=create_X_y (my_path,my_classes)\n","print (\"Nombre de données : \",X.shape[0])\n","print (\"Taille d'une image pour connaître l'input du réseau\", X[0].shape)\n","#print (\"Distribution des labels dans le jeu d'apprentissage\")\n","#sns.countplot(np.array(y))\n","#plt.title(\"Nombre d'éléments par classe\")\n","# affichage\n","plot_examples(X,y)\n","\n","# Surtout ne pas oublier de normaliser les données avec :\n","X=X.astype('float')\n","X=X/255.0"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WWM3nyCee6ze"},"outputs":[],"source":["# constantes globales\n","\n","IMG_SIZE=128\n","COLUMNS = 25 # Nombre d'images à afficher\n","\n","def get_optimizer_info(model):\n","    optimizer_config = model.optimizer.get_config()\n","    optimizer_name = model.optimizer.__class__.__name__  #Récupère le nom de l'optimiseur\n","    optimizer_params = f\"lr={optimizer_config['learning_rate']:.5f}, momentum={optimizer_config.get('momentum', 'N/A')}\"\n","\n","    return optimizer_name, optimizer_params\n","def save_to_csv(data, model, hyperparams=None, filename='/content/gdrive/MyDrive/Colab Notebooks/ML_FDS/resultats_evaluation.csv'):\n","    optimizer_name, optimizer_params = get_optimizer_info(model)\n","    file_exists = os.path.exists(filename)\n","    print(\">>> Chemin absolu utilisé :\", os.path.abspath(filename))\n","    print(\">>> Existe déjà ?\", os.path.exists(filename))\n","    with open(filename, mode='a', newline='') as file:\n","        writer = csv.writer(file)\n","\n","        if not file_exists:\n","            header = [\n","                'Folds', 'Epochs', 'Batch_Size',\n","                'Val_Loss', 'Val_Accuracy', 'Std_Accuracy',\n","                'Model_Type', 'Nom_Modele', 'Dropout_Rate',\n","                'Optimizer_Name', 'Optimizer_Params','Hyperparam'\n","            ]\n","            writer.writerow(header)\n","\n","        if hyperparams:\n","            hyperparams_str = ', '.join(f\"{k}={v}\" for k, v in hyperparams.items())\n","        else:\n","            hyperparams_str = \"NULL\"\n","\n","        row = data + [optimizer_name, optimizer_params, hyperparams_str]\n","        writer.writerow(row)\n","        file.flush()\n","        os.fsync(file.fileno())\n","        print(\"Fichier CSV attendu :\", os.path.abspath(filename))\n","        print(\"Existe déjà ?\", os.path.exists(filename))\n","        print(\"Ligne à écrire :\", row)\n","        print(\"Données enregistrées\")\n","\n","def build_dataframe_from_dir(data_dir, class_list):\n","    data = []\n","    for class_name in class_list:\n","        folder_path = os.path.join(data_dir, class_name)\n","        for fname in os.listdir(folder_path):\n","            if fname.lower().endswith('.jpg'):\n","                data.append({\n","                    'filepath': os.path.join(folder_path, fname),\n","                    'class': class_name\n","                })\n","    return pd.DataFrame(data)\n","\n","def evaluate_model(model_builder, dataX, dataY, folds=5, epochs=10, batch_size=16):\n","    scores, histories = [], []\n","    kfold = KFold(n_splits=folds, shuffle=True, random_state=42)\n","\n","    #K-fold\n","    for train_ix, test_ix in kfold.split(dataX):\n","        X_train, y_train = dataX[train_ix], dataY[train_ix]\n","        X_test, y_test = dataX[test_ix], dataY[test_ix]\n","        #Model vierge\n","        model = model_builder()\n","        history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size,\n","                            validation_data=(X_test, y_test), verbose=1)\n","        loss, acc = model.evaluate(X_test, y_test, verbose=0)\n","        print(f'Précision : {acc * 100:.3f}%')\n","\n","        scores.append(acc)\n","        histories.append(history)\n","\n","    return scores, histories\n","\n","def evaluate_model_from_dataframe(model_builder, df, folds=3, epochs=30, batch_size=16, target_size=(128, 128),class_list=None):\n","    scores, histories = [], []\n","    kfold = KFold(n_splits=folds, shuffle=True, random_state=42)\n","    print(\"appel nouveau evluate_model\")\n","    for train_idx, test_idx in kfold.split(df):\n","        train_df = df.iloc[train_idx]\n","        val_df = df.iloc[test_idx]\n","\n","        train_datagen = ImageDataGenerator(\n","          rescale=1./255,\n","          horizontal_flip=True,\n","          rotation_range=8,\n","          width_shift_range=0.15,\n","          height_shift_range=0.15,\n","          zoom_range=0.15,\n","        )\n","\n","        val_datagen = ImageDataGenerator(rescale=1./255)\n","\n","        train_generator = train_datagen.flow_from_dataframe(\n","            dataframe=train_df,\n","            x_col='filepath',\n","            y_col='class',\n","            classes=class_list, #super important pr avoir nos bonnes classes, sinon ça prend par ordre alpahébtique et on a une inversion\n","            target_size=target_size,\n","            class_mode='binary',\n","            batch_size=batch_size,\n","            shuffle=True\n","        )\n","        print(\"Mapping des classes Keras :\", train_generator.class_indices)\n","        val_generator = val_datagen.flow_from_dataframe(\n","            dataframe=val_df,\n","            x_col='filepath',\n","            y_col='class',\n","            classes=class_list, #idem\n","            target_size=target_size,\n","            class_mode='binary',\n","            batch_size=batch_size,\n","            shuffle=False\n","        )\n","        print(\"Mapping des classes Keras :\", val_generator.class_indices)\n","        #Model vierge\n","        model = model_builder()\n","\n","        history = model.fit(\n","            train_generator,\n","            validation_data=val_generator,\n","            steps_per_epoch=np.ceil(train_generator.samples / batch_size).astype(int),\n","            validation_steps=np.ceil(val_generator.samples / batch_size).astype(int),\n","            epochs=epochs,\n","            verbose=1\n","        )\n","\n","        loss, acc = model.evaluate(val_generator, verbose=0)\n","        print(f'Précision : {acc * 100:.3f}%')\n","\n","        scores.append(acc)\n","        histories.append(history)\n","\n","    return scores, histories\n","\n","def run_evaluation(folds, epochs,model_builder,X_animal=None,y_animal=None,best_params=None,model_path=None,save_final_model=True,batch_size=16,IDG=False,target_size=(128,128),df=None,class_list=None):\n","    model = model_builder()\n","    print(model.summary())\n","\n","    if IDG:\n","      print(\"utilise IDG\")\n","      scores, histories = evaluate_model_from_dataframe(\n","          model_builder=model_builder,\n","          df=df,\n","          folds=folds,\n","          epochs=epochs,\n","          batch_size=batch_size,\n","          target_size=target_size,\n","          class_list=class_list\n","      )\n","    else:\n","        scores, histories = evaluate_model(model_builder,X_animal, y_animal, folds, epochs)\n","    plot_curves(histories)\n","\n","    val_loss = np.mean([h.history['val_loss'][-1] for h in histories])\n","    # val_acc2 = np.mean([h.history['val_accuracy'][-1] for h in histories]) #Normalement faire ça pr recup la val_acc, mais j'me suis rendu compte de l'erruer\n","    # print(val_acc2)\n","    #pour une raison que j'ignore val_acc2 a tous le temps la même valeur que np.mean(scores)\n","    val_acc = np.mean(scores)  #Moyenne de la précision finale sur les K-folds\n","    #écart-type de la précision finale sur les K-folds\n","    std_acc = np.std(scores)\n","    data = [\n","        folds, epochs, batch_size,\n","        val_loss, val_acc, std_acc,\n","        'Convolutionnel', global_model_name, \"NULL\"\n","    ]\n","    if save_final_model and model_path:\n","      save_to_csv(data, model, hyperparams=best_params)\n","      final_model = model_builder()\n","      if IDG:\n","        datagen = ImageDataGenerator(rescale=1./255)\n","        full_generator = datagen.flow_from_dataframe(\n","            df,\n","            x_col='filepath',\n","            y_col='class',\n","            classes=class_list,\n","            target_size=target_size,\n","            class_mode='binary',\n","            batch_size=batch_size,\n","            shuffle=True\n","        )\n","        final_model.fit(full_generator, epochs=epochs)\n","      else:\n","        final_model.fit(X_animal, y_animal, epochs=epochs, batch_size=batch_size, verbose=1)\n","      final_model.save(model_path)\n","      print(f\"Modèle final sauvegardé sous : {model_path}\")\n","\n","    print(f'Précision finale : moyenne={mean(scores) * 100:.3f}% écart-type={std(scores) * 100:.3f}%, k={len(scores)}')"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"RokfWPznxCtw"},"outputs":[],"source":["global_model_name = input(\"Veuillez insérer le nom de modèle (sans le .keras): \")\n","global_model_name += '.keras'\n","save_dir = \"saved_models/\"\n","os.makedirs(save_dir, exist_ok=True)\n","model_path = os.path.join(save_dir, global_model_name)\n","animals = {\"0\": \"tiger\",\"1\": \"fox\",\"2\": \"elephant\"}\n","animal_choix = input(\"Veuillez insérer le numéro de l'animal voulu (0=tiger, 1=fox, 2=elephant) : \")\n","if animal_choix not in animals:\n","    raise ValueError(\"Choix invalide. Veuillez entrer 0, 1 ou 2.\")\n","selected_animal = animals[animal_choix]\n","class_list = [selected_animal, f\"{selected_animal.capitalize()}_negative_class\"]\n","X_animal, y_animal = create_X_y(\"Data_Project/Tiger-Fox-Elephant/\", class_list)\n","X_animal = X_animal.astype('float32') / 255.0 #Ici j'ai mis float32 pour avoir moins d'octet et utiliser moins de ressources (16,32,64 possible)\n","\n","def build_baseline_model(shapeinput=(128, 128, 3), learning_rate=0.01, momentum=0.9):\n","    model = Sequential()\n","    model.add(Input(shape=shapeinput, name=\"Input_Layer\"))\n","\n","    #Couche convolutionnelle avec pooling (1 couche CNN)\n","    model.add(Conv2D(filters=32, kernel_size=(3, 3), activation='relu', name=\"Conv2D_1\"))\n","    model.add(MaxPooling2D(pool_size=(2, 2), name=\"MaxPooling2D_1\"))\n","\n","    #Flatten pour préparer les données pour la partie dense\n","    model.add(Flatten(name=\"Flatten\"))\n","\n","    #Couches denses pour la classification\n","    model.add(Dense(1, activation='sigmoid', name=\"Output\"))\n","\n","    opt = SGD(learning_rate=0.001, momentum=momentum)\n","    model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy'])\n","\n","    return model\n","\n","# k=3 # Nombre de folds\n","# epochs=100 # Nombre d'epochs\n","# # run_evaluation(k,epochs,build_baseline_model,\n","# #                X_animal,y_animal,\n","# #                model_path=model_path,save_final_model=False, #mettre a True si on veut save le modèle final + dans le CSV, False sinon\n","# #                );\n","# run_evaluation(k,epochs,build_baseline_model,\n","#                model_path=model_path,save_final_model=True,\n","#                df=build_dataframe_from_dir(\"Data_Project/Tiger-Fox-Elephant/\", class_list),\n","#                batch_size=16,IDG=True,\n","#                target_size=(128,128),\n","#                class_list=class_list\n","#                )"]},{"cell_type":"markdown","source":["Pour des raisons liées à l'empreinte carbone et à la consommation énergétique, l'entraînement du modèle a été désactivé : vous pouvez simplement charger le modèle préentraîné.\n","Les modèles sont disponible sur notre git\n","\n","https://github.com/NicolasCani/ProjetL3"],"metadata":{"id":"ORTmM_FbCmAI"}},{"cell_type":"code","source":["class_list = [\"fox\", f\"Fox_negative_class\"]\n","\n","X_test, y_test = create_X_y(\"Data_Project/Tiger-Fox-Elephant/\", class_list)\n","X_test = X_test.astype('float32') / 255.0 #Ici j'ai mis float32 pour avoir moins d'octet et utiliser moins de ressources (16,32,64 possible)\n","\n","# Chargement du modèle sauvegardé\n","model = load_model('/content/gdrive/MyDrive/Colab Notebooks/ML_FDS/saved_models/FoxBaseline.keras') # a modifié en fonction de celui que vous souhaitez\n","\n","# Évaluation du modèle sur le jeu de test\n","test_loss, test_acc = model.evaluate(X_test, y_test, verbose=2)\n","\n","# Affichage des résultats de test\n","print(f'Loss sur le jeu de test : {test_loss *100:.2f}')\n","print(f'Accuracy sur le jeu de test : {test_acc * 100:.2f}%')\n","\n","# Prédiction sur le jeu de test\n","y_pred = model.predict(X_test)\n","\n","# Calcul de la matrice de confusion\n","conf = confusion_matrix(y_test, np.round(y_pred).astype(int))\n","\n","# Affichage de la matrice de confusion\n","plot_confusion_matrix(conf, [\"Fox_negative_class\", \"fox\"])\n"],"metadata":{"id":"CrXTMeV1Clje"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MXeQeHECI6F9","collapsed":true},"outputs":[],"source":["global_model_name = input(\"Veuillez insérer le nom de modèle (sans le .keras): \")\n","global_model_name += '.keras'\n","save_dir = \"saved_models/\"\n","os.makedirs(save_dir, exist_ok=True)\n","model_path = os.path.join(save_dir, global_model_name)\n","animals = {\"0\": \"tiger\",\"1\": \"fox\",\"2\": \"elephant\"}\n","animal_choix = input(\"Veuillez insérer le numéro de l'animal voulu (0=tiger, 1=fox, 2=elephant) : \")\n","if animal_choix not in animals:\n","    raise ValueError(\"Choix invalide. Veuillez entrer 0, 1 ou 2.\")\n","selected_animal = animals[animal_choix]\n","class_list = [selected_animal, f\"{selected_animal.capitalize()}_negative_class\"]\n","X_animal, y_animal = create_X_y(\"Data_Project/Tiger-Fox-Elephant/\", class_list)\n","X_animal = X_animal.astype('float32') / 255.0\n","\n","def build_model_Q2(shapeinput=(128, 128, 3),\n","                num_cnn_layers=2,filters_list=[32, 64],kernel_sizes=[(3, 3), (3, 3)],\n","                num_dense_layers=2, dense_units_list=[64, 32],\n","                dropout_rate=0.3,\n","                learning_rate=0.01,\n","                momentum=0.9,\n","                   ):\n","    print(\"Paramètres (choisis par Optuna):\")\n","    print(\"learning_rate :\", learning_rate)\n","    print(\"momentum :\", momentum)\n","    print(\"dropout_rate :\", dropout_rate)\n","    print(\"CNN layers :\", num_cnn_layers, filters_list, kernel_sizes)\n","    print(\"Dense layers :\", num_dense_layers, dense_units_list)\n","    print(\"--------------\")\n","\n","    model = Sequential()\n","    model.add(Input(shape=shapeinput, name=\"Input_Layer\"))\n","\n","    for i in range(num_cnn_layers):\n","        model.add(Conv2D(filters=filters_list[i], kernel_size=kernel_sizes[i], activation='relu', name=f\"Conv2D_{i}\"))\n","        model.add(MaxPooling2D(pool_size=(2, 2), name=f\"MaxPooling2D_{i}\"))\n","    model.add(Flatten(name=\"Flatten\"))\n","\n","    for i in range(num_dense_layers):\n","        model.add(Dense(dense_units_list[i], activation='relu', name=f\"Dense_{i+1}\"))\n","        model.add(Dropout(dropout_rate, name=f\"Dropout_{i+1}\"))\n","\n","    model.add(Dense(1, activation='sigmoid', name=\"Output\"))\n","\n","    opt = SGD(learning_rate=learning_rate, momentum=momentum)\n","    model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy'])\n","    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n","\n","    return model\n","\n","def objective(trial):\n","    learning_rate = 0.001#a augementer si on utilise pas de IDG a réduire sinon\n","    momentum = trial.suggest_categorical('momentum', [0.6, 0.7, 0.8, 0.9, 0.95])\n","    dropout_rate = trial.suggest_categorical('dropout_rate', [0.1, 0.2, 0.3, 0.4, 0.5])\n","\n","    num_cnn_layers = trial.suggest_int('num_cnn_layers', 1, 3)\n","    filters_list = [trial.suggest_categorical(f'filters_{i}', [16, 32, 64]) for i in range(num_cnn_layers)]\n","    kernel_sizes = [trial.suggest_categorical(f'kernel_{i}', [(3, 3), (5, 5)]) for i in range(num_cnn_layers)]\n","\n","    num_dense_layers = trial.suggest_int('num_dense_layers', 1, 3)\n","    dense_units_list = [trial.suggest_categorical(f'units_{i}', [32, 64]) for i in range(num_dense_layers)]\n","\n","    print(\"Paramètres choisis par Optuna :\")\n","    print(\"learning_rate :\", learning_rate)\n","    print(\"momentum :\", momentum)\n","    print(\"dropout_rate :\", dropout_rate)\n","    print(\"CNN layers :\", num_cnn_layers, filters_list, kernel_sizes)\n","    print(\"Dense layers :\", num_dense_layers, dense_units_list)\n","    print(\"--------------\")\n","\n","    def model_fn():\n","        return build_model_Q2(\n","            learning_rate=learning_rate,\n","            momentum=momentum,\n","            dropout_rate=dropout_rate,\n","            num_cnn_layers=num_cnn_layers,\n","            filters_list=filters_list,\n","            kernel_sizes=kernel_sizes,\n","            num_dense_layers=num_dense_layers,\n","            dense_units_list=dense_units_list\n","        )\n","\n","    scores, _ = evaluate_model(model_fn, X_animal, y_animal, folds=k, epochs=epochs) #MODIFIER ICI SI BESOIN D'UTILISER IDG FAIRE LE BON APPEL\n","\n","    return np.mean(scores)\n","\n","# k=3 # Nombre de folds\n","# epochs=60 # Nombre d'epochs\n","# # Lancer l’opti\n","# print(\"lancement de l'opti\")\n","# study = optuna.create_study(direction='maximize')\n","# study.optimize(objective, n_trials=10) # Nombre d'essaye a set ici\n","\n","# best_params = study.best_params\n","# def best_model_fn():\n","#     return build_model_Q2(\n","#         learning_rate=0.001, #a augementer si on utilise pas de IDG a réduire sinon\n","#         momentum=best_params['momentum'],\n","#         dropout_rate=best_params['dropout_rate'],\n","#         num_cnn_layers=best_params['num_cnn_layers'],\n","#         filters_list=[best_params[f'filters_{i}'] for i in range(best_params['num_cnn_layers'])],\n","#         kernel_sizes=[best_params[f'kernel_{i}'] for i in range(best_params['num_cnn_layers'])],\n","#         num_dense_layers=best_params['num_dense_layers'],\n","#         dense_units_list=[best_params[f'units_{i}'] for i in range(best_params['num_dense_layers'])]\n","#     )\n","\n","# print(\"Meilleur score :\", study.best_value)\n","# print(\"Meilleurs hyperparamètres :\")\n","# for key, value in best_params.items():\n","#     print(f\" - {key}: {value}\")\n","\n","# # run_evaluation(X_animal,y_animal,k, epochs,\n","# #                best_model_fn,best_params=study.best_params, #passage de la fonction qui crée le modele avec les meilleurs hyperparam\n","# #                model_path=model_path,\n","# #                save_final_model=False, #mettre a True si on veut save le modèle final + dans le CSV, False sinon\n","# #                );\n","# run_evaluation(k,epochs,best_model_fn,best_params=best_params,\n","#                model_path=model_path,save_final_model=True,\n","#                df=build_dataframe_from_dir(\"Data_Project/Tiger-Fox-Elephant/\", class_list),\n","#                batch_size=16,IDG=True,\n","#                target_size=(128,128)\n","#                )"]},{"cell_type":"markdown","source":["\n","\n","Pour des raisons liées à l'empreinte carbone et à la consommation énergétique, l'entraînement du modèle a été désactivé : vous pouvez simplement charger le modèle préentraîné. Les modèles sont disponible sur notre git\n","\n","https://github.com/NicolasCani/ProjetL3\n"],"metadata":{"id":"5oC_3r4TG1Ck"}},{"cell_type":"code","source":["class_list = [\"fox\", f\"Fox_negative_class\"]\n","\n","X_test, y_test = create_X_y(\"Data_Project/Tiger-Fox-Elephant/\", class_list)\n","X_test = X_test.astype('float32') / 255.0 #Ici j'ai mis float32 pour avoir moins d'octet et utiliser moins de ressources (16,32,64 possible)\n","\n","# Chargement du modèle sauvegardé\n","model = load_model('/content/gdrive/MyDrive/Colab Notebooks/ML_FDS/saved_models/FoxEtenduV2.keras') # a modifié en fonction de celui que vous souhaitez\n","\n","# Évaluation du modèle sur le jeu de test\n","test_loss, test_acc = model.evaluate(X_test, y_test, verbose=2)\n","\n","# Affichage des résultats de test\n","print(f'Loss sur le jeu de test : {test_loss *100:.2f}')\n","print(f'Accuracy sur le jeu de test : {test_acc * 100:.2f}%')\n","\n","# Prédiction sur le jeu de test\n","y_pred = model.predict(X_test)\n","\n","# Calcul de la matrice de confusion\n","conf = confusion_matrix(y_test, np.round(y_pred).astype(int))\n","\n","# Affichage de la matrice de confusion\n","plot_confusion_matrix(conf, [\"Fox_negative_class\", \"fox\"])\n"],"metadata":{"id":"jAD86D-gHExX"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tG3NCh-qa-Nz"},"outputs":[],"source":["#code pour voir les images bien/mal classé\n","nom_Modele = input(\"Veuillez écrire le nom du modèle (sans le .keras) : \")\n","model = load_model(f'saved_models/{nom_Modele}.keras')\n","\n","animals = {\"0\": \"tiger\", \"1\": \"fox\", \"2\": \"elephant\"}\n","animal_choix = input(\"Veuillez insérer le numéro de l'animal voulu (0=tiger, 1=fox, 2=elephant) : \")\n","if animal_choix not in animals:\n","    raise ValueError(\"Choix invalide. Veuillez entrer 0, 1 ou 2.\")\n","selected_animal = animals[animal_choix]\n","\n","class_list = [selected_animal, f\"{selected_animal.capitalize()}_negative_class\"]\n","X, y = create_X_y(\"Data_Project/Tiger-Fox-Elephant/\", class_list)\n","X = X.astype('float32') / 255.0\n","y = np.asarray(y)\n","\n","\n","pred_probs = model.predict(X)\n","prediction = (pred_probs > 0.5).astype(int).flatten()\n","\n","misclassified = np.where(y != prediction)[0]\n","\n","print(f\"Nb d'images mal classées : {len(misclassified)}\")\n","print(\"Les images mal classées sont :\")\n","\n","for i in misclassified:\n","    true_label = class_list[y[i]]\n","    predicted_label = class_list[prediction[i]]\n","    print(f\"index: {i}, réel: {true_label}, prédiction: {predicted_label}\")\n","\n","print(\"Images mal classées :\")\n","for i in misclassified:\n","    img = (X[i] * 255).astype(np.uint8)\n","    img_rgb = img[:, :, ::-1]\n","    plt.imshow(img_rgb)\n","    plt.title(f\"index {i} | réel: {class_list[y[i]]} | prédiction: {class_list[prediction[i]]}\")\n","    plt.axis(\"off\")\n","    plt.show()"]},{"cell_type":"markdown","source":["```python\n","tf.keras.preprocessing.image.ImageDataGenerator(\n","    featurewise_center=False,                # Centrer les données en soustrayant la moyenne\n","    samplewise_center=False,                 # Centrer chaque échantillon\n","    featurewise_std_normalization=False,     # Normaliser les données en divisant par l'écart-type\n","    samplewise_std_normalization=False,      # Normaliser chaque échantillon\n","    zca_whitening=False,                     # Appliquer le blanchiment ZCA\n","    zca_epsilon=1e-06,                      # Epsilon pour le blanchiment ZCA\n","    rotation_range=0,                        # Plage de degrés pour les rotations aléatoires\n","    width_shift_range=0.0,                  # Fraction de la largeur totale pour les décalages horizontaux aléatoires\n","    height_shift_range=0.0,                 # Fraction de la hauteur totale pour les décalages verticaux aléatoires\n","    brightness_range=None,                   # Plage pour les ajustements aléatoires de luminosité\n","    shear_range=0.0,                         # Angle de cisaillement dans le sens antihoraire\n","    zoom_range=0.0,                          # Plage pour le zoom aléatoire\n","    channel_shift_range=0.0,                 # Décalage des canaux de couleur\n","    fill_mode='nearest',                     # Les points en dehors des limites sont remplis selon le mode choisi\n","    cval=0.0,                                # Valeur utilisée pour les points en dehors des limites lorsque fill_mode est 'constant'\n","    horizontal_flip=False,                   # Retourner aléatoirement les entrées horizontalement\n","    vertical_flip=False,                     # Retourner aléatoirement les entrées verticalement\n","    rescale=None,                            # Facteur de mise à l'échelle\n","    preprocessing_function=None,             # Fonction qui sera appliquée à chaque image\n","    data_format=None,                        # Format des données d'image, soit 'channels_last' soit 'channels_first'\n","    validation_split=0.0,                   # Fraction des données réservée pour la validation\n","    dtype=None                               # Type de données\n",")\n"],"metadata":{"id":"T9Gl0GBe3Hwc"}},{"cell_type":"code","source":["#code juste pour voir les images\n","animals = {\"0\": \"tiger\",\"1\": \"fox\",\"2\": \"elephant\"}\n","animal_choix = input(\"Veuillez insérer le numéro de l'animal voulu (0=tiger, 1=fox, 2=elephant) : \")\n","if animal_choix not in animals:\n","    raise ValueError(\"Choix invalide. Veuillez entrer 0, 1 ou 2.\")\n","selected_animal = animals[animal_choix]\n","class_list = [selected_animal, f\"{selected_animal.capitalize()}_negative_class\"]\n","\n","X_animal, y_animal = create_X_y(\"Data_Project/Tiger-Fox-Elephant/\", class_list)\n","\n","datagen = ImageDataGenerator(\n","      horizontal_flip=True,\n","      rotation_range=3,\n","      width_shift_range=0.02,\n","      height_shift_range=0.02,\n","      zoom_range=0.15,\n",")\n","\n","nb_images = 10\n","\n","nb_genere = 0\n","batch_size = 5\n","\n","for X_batch, y_batch in datagen.flow(X_animal, y_animal, batch_size=batch_size, shuffle=True):\n","    for i in range(len(X_batch)):\n","        img = X_batch[i]\n","        label = y_batch[i]\n","\n","        class_name = class_list[label] # 0 dans le cas où classe 0 (donc tigre) 1 si non tigre\n","\n","        img_rgb = img.astype(np.uint8)\n","        img_rgb = img_rgb[:, :, ::-1]  # Sinon les tigres sont bleues lol\n","        image_pil = Image.fromarray(img_rgb)\n","\n","        # Sauvegarde\n","        plt.imshow(img_rgb)\n","        plt.title(f\"Image Générée {nb_genere} - Classe: {class_name}\")\n","        plt.axis(\"off\")\n","        plt.show()\n","\n","        nb_genere += 1\n","        if nb_genere >= nb_images:\n","            break\n","    if nb_genere >= nb_images:\n","        break"],"metadata":{"id":"UYi9CEcGV2zf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["global_model_name = input(\"Veuillez insérer le nom de modèle (sans le .keras): \")\n","global_model_name += '.keras'\n","save_dir = \"saved_models/\"\n","os.makedirs(save_dir, exist_ok=True)\n","model_path = os.path.join(save_dir, global_model_name)\n","animals = {\"0\": \"tiger\", \"1\": \"fox\", \"2\": \"elephant\"}\n","animal_choix = input(\"Veuillez insérer le numéro de l'animal voulu (0=tiger, 1=fox, 2=elephant) : \")\n","if animal_choix not in animals:\n","    raise ValueError(\"Choix invalide. Veuillez entrer 0, 1 ou 2.\")\n","\n","selected_animal = animals[animal_choix]\n","class_list = [selected_animal, f\"{selected_animal.capitalize()}_negative_class\"]\n","X_animal, y_animal = create_X_y(\"Data_Project/Tiger-Fox-Elephant/\", class_list)\n","X_animal = X_animal.astype('float32') / 255.0 #Ici j'ai mis float32 pour avoir moins d'octet et utiliser moins de ressources (16,32,64 possible)\n","\n","def modelresnet():\n","  input_shape_animal = (128, 128, 3)\n","  input_shape_resnet = (224, 224, 3)\n","  model = Sequential()\n","\n","  model.add(Input(shape=input_shape_animal, name=\"Input_Layer\"))\n","\n","  # Ajouter une couche Lambda pour redimensionner les images\n","  model.add(Lambda(lambda image: tf.image.resize(image, input_shape_resnet[:2]), name=\"Lambda_Layer\"))  # Resize to (224, 224)\n","  # Ajouter ResNet50 sans la couche de classification\n","  model.add(DenseNet121(include_top=False, pooling='avg', weights='imagenet',classes=2,name=\"DenseNet121_Layer\"))\n","  model.get_layer(\"DenseNet121_Layer\").trainable = False\n","  # Ajout de la partie classification\n","  model.add(Flatten())\n","  model.add(Dense(1, activation='sigmoid', name=\"Output\"))\n","  # Compiler le modèle\n","  opt = SGD(learning_rate=0.01)\n","  model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy'])\n","\n","  # Afficher le résumé du modèle\n","  return model\n","\n","# k=3 # Nombre de folds\n","# epochs=60 # Nombre d'epochs\n","# # run_evaluation(k, epochs,modelresnet,\n","# #                X_animal,y_animal,\n","# #                model_path=model_path,save_final_model=False, #mettre a True si on veut save le modèle final + dans le CSV, False sinon\n","# #                );\n","# # model=modelresnet()\n","# # model.get_layer(\"ResNet_Layer\").trainable = False\n","# # print(model.summary())\n","# # history = model.fit(X_animal, y_animal, epochs=epochs, batch_size=64,\n","# #                             validation_data=(X_animal, y_animal), verbose=1)\n","# # plot_curves([history])\n","# run_evaluation(k,epochs,modelresnet,\n","#                model_path=model_path,save_final_model=True,\n","#                df=build_dataframe_from_dir(\"Data_Project/Tiger-Fox-Elephant/\", class_list),\n","#                batch_size=16,IDG=True,\n","#                target_size=(224,224) #important ça !\n","#                )"],"metadata":{"id":"u98nktVFmXrw","collapsed":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["PARTIE AUTOENCODEUR"],"metadata":{"id":"QXE1ecoMEEXx"}},{"cell_type":"code","source":["# Importation des différentes librairies utiles pour le notebook\n","\n","# Sickit-learn met régulièrement à jour des versions et\n","# indique des futurs warnings. Ces deux lignes permettent de ne pas les afficher.\n","import warnings  # Permet de gérer les avertissements\n","warnings.filterwarnings(\"ignore\", category=FutureWarning)\n","\n","# Librairies générales\n","import pickle  # Pour la sérialisation et la désérialisation d'objets Python\n","import pandas as pd  # Manipulation de données en format tabulaire\n","from scipy.stats import randint  # Pour les distributions aléatoires\n","import numpy as np  # Calcul numérique\n","from numpy import mean, std  # Statistiques de base\n","import string  # Manipulation des chaînes de caractères\n","import os  # Interaction avec le système d'exploitation\n","from os import listdir  # Liste les fichiers dans un répertoire\n","from os.path import isfile, join  # Fonctions pour gérer les chemins de fichiers\n","import time  # Gestion du temps\n","import base64  # Encodage et décodage en base64\n","import re  # Expressions régulières\n","import sys  # Manipulation du système\n","import copy  # Copie d'objets Python\n","import random  # Génération de nombres aléatoires\n","import datetime  # Manipulation des dates et heures\n","import glob  # Recherche de fichiers avec des motifs\n","from glob import glob  # Pour la recherche de fichiers avec des motifs\n","import cv2  # Librairie pour la vision par ordinateur\n","import zipfile  # Manipulation des archives ZIP\n","\n","# Librairie d'affichage\n","import matplotlib.pyplot as plt  # Visualisation de données\n","from PIL import Image, ImageDraw  # Manipulation d'images avec Pillow\n","\n","# Scikit-learn pour l'apprentissage automatique\n","from sklearn.metrics import confusion_matrix, accuracy_score  # Métriques pour l'évaluation des modèles\n","from sklearn.model_selection import KFold, cross_val_score, train_test_split  # Séparation des données et validation croisée\n","from sklearn.pipeline import Pipeline  # Création de pipelines pour les modèles\n","from sklearn.decomposition import PCA  # Analyse en composantes principales (ACP)\n","\n","# TensorFlow et Keras pour le deep learning\n","import tensorflow as tf  # Framework principal pour l'apprentissage profond\n","import keras  # API de haut niveau pour TensorFlow\n","from keras import layers  # Modules de construction de modèles Keras\n","from keras import models  # Définition et manipulation des modèles\n","from keras import optimizers  # Optimisateurs pour les modèles Keras\n","#from keras.preprocessing.image import ImageDataGenerator  # Déprécié, remplacé par TensorFlow\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator  # Générateur d'images pour l'augmentation des données\n","\n","# Utilitaires Keras pour le traitement d'images\n","from keras.utils import img_to_array, load_img  # Conversion d'images en tableaux\n","from keras.callbacks import ModelCheckpoint, EarlyStopping  # Pour la gestion des checkpoints et de l'arrêt précoce\n","from keras import backend as K  # Backend Keras pour des opérations de bas niveau\n","from keras.models import Sequential, Model, load_model  # Définition et chargement des modèles Keras\n","from keras.layers import LeakyReLU  # Activation LeakyReLU\n","from keras.preprocessing import image  # Prétraitement des images\n","\n","# Couches Keras pour les modèles convolutifs et autres\n","from keras.layers import Input, Conv2D, Flatten, Dense, Conv2DTranspose, Reshape, Lambda, Activation, BatchNormalization, LeakyReLU, Dropout  # Couches pour CNN et autoencoders\n","from keras.layers import InputLayer, MaxPooling2D, UpSampling2D  # Couches supplémentaires pour les CNN\n","from keras.optimizers import Adam  # Optimiseur Adam\n","from keras.utils import plot_model  # Visualisation du modèle\n","import tensorflow as tf\n","from PIL import Image, ImageDraw\n","import numpy as np\n","import random\n","from glob import glob\n"],"metadata":{"id":"dAtYd-Q2FNs1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Définition des paramètres principaux\n","DATA_FOLDER = \"Data_Project/Tiger-Fox-Elephant\"\n","WEIGHTS_FOLDER = './myweights/AE'\n","WEIGHTS_FOLDER2 = './myweights/AENoise'\n","WEIGHTS_FOLDER3 = './myweights/AENB'\n","WEIGHTS_FOLDER4 = './myweights/AEElephant'\n","WEIGHTS_FOLDER5 = './myweights/AETiger'\n","\n","\n","# Calcul du nombre d'images dans le dossier spécifié\n","filenames = np.array(glob(os.path.join(DATA_FOLDER+'/fox', '*.jpg')) + glob(os.path.join(DATA_FOLDER+'/fox', '*.jpeg')))\n","NUM_IMAGES = len(filenames)\n","print(NUM_IMAGES)\n","print(\"Total number of images : \" + str(NUM_IMAGES))\n","\n","# Création du répertoire pour sauvegarder les poids, s'il n'existe pas\n","os.makedirs(WEIGHTS_FOLDER, exist_ok=True)\n","os.makedirs(WEIGHTS_FOLDER2, exist_ok=True)\n","os.makedirs(WEIGHTS_FOLDER3, exist_ok=True)\n","os.makedirs(WEIGHTS_FOLDER4, exist_ok=True)\n","os.makedirs(WEIGHTS_FOLDER5, exist_ok=True)\n","\n","def add_mask_on_image(image):\n","    \"\"\"\n","    Ajoute un masque sous forme de carré noir sur l'image donnée.\n","\n","    Le masque est un carré de taille fixe, positionné aléatoirement sur l'image. Ce carré\n","    cache une partie de l'image originale.\n","\n","    Paramètres:\n","    -----------\n","    image : PIL.Image\n","        L'image sur laquelle le masque sera ajouté. Doit être une image au format RGB.\n","\n","    Retour:\n","    -------\n","    numpy.ndarray\n","        L'image modifiée avec un masque noir ajouté sous forme de carré.\n","    \"\"\"\n","    size_square = 42  # Taille du carré\n","    output_img = image\n","    random_start_point = np.random.randint(0, 100)  # Point de départ aléatoire pour le carré\n","    img_draw = ImageDraw.Draw(output_img)\n","    # Dessiner le carré noir\n","    img_draw.rectangle([(random_start_point, random_start_point),\n","                        (random_start_point + size_square, random_start_point + size_square)], fill='black')\n","    return np.array(output_img)\n","\n","\n","def add_noise_on_image(image):\n","    \"\"\"\n","    Ajoute du bruit aléatoire à l'image.\n","\n","    Le bruit est généré selon une distribution normale et est ajouté à l'image.\n","    Plus le niveau de bruit est élevé, plus l'image est déformée.\n","\n","    Paramètres:\n","    -----------\n","    image : PIL.Image\n","        L'image à laquelle du bruit sera ajouté. Doit être une image au format RGB.\n","\n","    Retour:\n","    -------\n","    PIL.Image\n","        L'image bruitée.\n","    \"\"\"\n","    level_of_noise = 4  # Niveau de bruit. Plus il est élevé, plus la déformation est importante\n","    # Convertir l'image en tableau NumPy\n","    image_array = np.array(image)\n","    # Générer du bruit aléatoire avec la même forme que l'image\n","    noise = np.random.normal(loc=0, scale=25 * level_of_noise, size=image_array.shape)\n","    # Ajouter le bruit à l'image\n","    noisy_image_array = image_array + noise\n","    # Limiter les valeurs pour qu'elles soient dans la plage valide (0-255)\n","    noisy_image_array = np.clip(noisy_image_array, 0, 255)\n","    # Convertir le tableau NumPy en image\n","    noisy_image = Image.fromarray(noisy_image_array.astype(np.uint8))\n","    return noisy_image\n","\n","\n","def black_and_white_on_image(image):\n","    \"\"\"\n","    Convertit l'image en niveaux de gris et redimensionne à 128x128.\n","\n","    Cette fonction prend une image en couleur, la convertit en niveaux de gris et\n","    la redimensionne à une taille fixe de 128x128.\n","\n","    Paramètres:\n","    -----------\n","    image : PIL.Image\n","        L'image à convertir en noir et blanc. Doit être une image au format RGB.\n","\n","    Retour:\n","    -------\n","    numpy.ndarray\n","        L'image en niveaux de gris redimensionnée à 128x128 et convertie en tableau NumPy.\n","    \"\"\"\n","    # Convertir l'image en niveaux de gris\n","    grayscale_image = image.convert(\"L\")\n","    # Redimensionner l'image en niveaux de gris à la taille souhaitée (128x128)\n","    grayscale_image = grayscale_image.resize((128, 128))\n","    # Convertir l'image en niveaux de gris en tableau NumPy pour l'affichage avec matplotlib\n","    image_array = np.array(grayscale_image)\n","    # Empiler le tableau en niveaux de gris pour créer une image en niveaux de gris à 3 canaux\n","    image_array = np.stack((image_array,) * 3, axis=-1)\n","    return image_array\n","\n","\n","def my_image_generator(folder, classes, target_size, batch_size, typeimage='mask', shuffle=True):\n","    \"\"\"\n","    Génère un batch d'images corrompues et originales pour l'entraînement d'un modèle.\n","\n","    En fonction du type d'image spécifié (mask, noise, blackwhite), la fonction génère des\n","    images corrompues en ajoutant des masques, du bruit ou en convertissant les images en noir et blanc.\n","    Les images originales sont également renvoyées pour l'entraînement supervisé.\n","\n","    Paramètres:\n","    -----------\n","    folder : str\n","        Le répertoire où se trouvent les sous-dossiers d'images de chaque classe.\n","    classes : list\n","        Liste des classes d'images (chaque classe étant un sous-dossier dans `folder`).\n","    target_size : tuple\n","        Taille cible des images redimensionnées (hauteur, largeur).\n","    batch_size : int\n","        Taille du batch (nombre d'images à traiter par itération).\n","    typeimage : str, optionnel\n","        Type de transformation à appliquer sur les images corrompues. Les valeurs possibles sont :\n","        - 'mask' : ajouter un masque sous forme de carré.\n","        - 'noise' : ajouter du bruit aléatoire.\n","        - 'blackwhite' : convertir l'image en noir et blanc.\n","    shuffle : bool, optionnel\n","        Si True, les fichiers sont mélangés avant d'être utilisés.\n","\n","    Retour:\n","    -------\n","    tuple\n","        Un tuple contenant deux éléments :\n","        - Un tableau numpy des images corrompues.\n","        - Un tableau numpy des images originales.\n","    \"\"\"\n","    # Créer une liste de chemins de fichiers\n","    files = []\n","    for c in classes:\n","        class_folder = folder + '/' + c\n","        file_pattern = class_folder + '/*.jpg'  # Mettre à jour l'extension des fichiers pour correspondre à vos images\n","        files.extend(glob(file_pattern))\n","\n","    # Mélanger la liste des chemins de fichiers si shuffle est True\n","    if shuffle:\n","        random.shuffle(files)\n","\n","    # Initialiser le compteur d'itérations\n","    iter = 0\n","\n","    while True:\n","        # Vérifier si l'index des fichiers est invalide, si oui réinitialiser\n","        if (iter * batch_size + batch_size) > len(files):\n","            iter = 0\n","        end_iter = iter * batch_size + batch_size\n","        # Sélectionner les fichiers pour le batch\n","        paths = files[iter * batch_size:end_iter]\n","        # Lire chaque image et effectuer le prétraitement pour le batch d'images\n","        corrupted_images_batch = []\n","        original_images_batch = []\n","        for path in paths:\n","            img = Image.open(path)\n","            # Ne considérer que les images avec 3 canaux (RGB)\n","            if img.mode == 'RGB':\n","                # Redimensionner l'image à la taille cible\n","                img = img.resize(target_size, resample=Image.BILINEAR)\n","\n","                original_images_batch.append(np.array(img) / 255)\n","\n","                # Vérifier la taille de l'image\n","                if img.size[0] == target_size[0] and img.size[1] == target_size[1]:\n","                    if typeimage == 'mask':\n","                        changed_img = add_mask_on_image(img)\n","                    elif typeimage == 'noise':\n","                        changed_img = add_noise_on_image(img)\n","                    elif typeimage == 'blackwhite':\n","                        changed_img = black_and_white_on_image(img)\n","                else:\n","                    print(\"L'image {} n'a pas les dimensions appropriées.\".format(path))\n","\n","                preprocess_img = np.array(changed_img)\n","                corrupted_images_batch.append(preprocess_img / 255)\n","            else:\n","                continue\n","        # Retourner un tuple de (batch_images_corrupted, batch_images_originales) pour alimenter le réseau\n","        corrupted_images_batch = np.array(corrupted_images_batch)\n","        original_images_batch = np.array(original_images_batch)\n","        # Passer au batch suivant\n","        iter = iter + 1\n","\n","        yield (corrupted_images_batch, original_images_batch)\n","def display_images_from_batch(batch, typeimage):\n","    \"\"\"\n","    Affiche un batch d'images corrompues et originales sous forme de sous-graphes.\n","\n","    Cette fonction affiche les 8 premières images du batch donné, en comparant les images originales\n","    (non modifiées) avec les images corrompues (modifiées). Elle prend en charge les images en noir et blanc\n","    en ajustant le colormap ('gray').\n","\n","    Paramètres:\n","    -----------\n","    batch : tuple\n","        Un tuple contenant deux éléments :\n","        - Le premier élément est une liste ou un tableau d'images corrompues (par exemple, avec des trous, du bruit, etc.).\n","        - Le second élément est une liste ou un tableau d'images originales, correspondant aux images non modifiées.\n","        Les deux éléments doivent être des tableaux de forme (batch_size, hauteur, largeur, canaux).\n","\n","    typeimage : str\n","        Un indicateur de type d'image :\n","        - 'blackwhite' : pour afficher les images corrompues en niveaux de gris.\n","        - Toute autre valeur : pour afficher les images corrompues avec les couleurs d'origine.\n","\n","    Retour:\n","    -------\n","    None\n","        La fonction affiche les images, mais ne retourne rien.\n","\n","    Exemples:\n","    ---------\n","    1. Pour afficher les premières 8 images d'un batch avec des images corrompues en noir et blanc :\n","       display_images_from_batch(batch, typeimage='blackwhite')\n","\n","    2. Pour afficher les premières 8 images d'un batch avec des images corrompues en couleur :\n","       display_images_from_batch(batch, typeimage='color')\n","    \"\"\"\n","\n","    # Extraire les 8 premières images corrompues du batch\n","    corrupted_images = batch[0][:8]\n","    # Extraire les 8 premières images originales du batch\n","    original_images = batch[1][:8]\n","\n","    # Créer une figure avec 8 lignes et 2 colonnes pour afficher les images\n","    fig, axs = plt.subplots(8, 2, figsize=(8, 16))  # Ajustez la taille de la figure\n","    fig.tight_layout(pad=2.0)  # Ajuster l'espacement entre les sous-graphes\n","\n","    for i in range(8):\n","        # Afficher l'image originale dans la première colonne\n","        axs[i, 0].imshow(original_images[i].squeeze())\n","        axs[i, 0].set_title('Image Originale')  # Titre de la première colonne\n","        axs[i, 0].axis('off')  # Masquer les axes\n","\n","        # Si l'image est en noir et blanc, utiliser le cmap 'gray' pour l'affichage\n","        if typeimage == 'blackwhite':\n","            axs[i, 1].imshow(corrupted_images[i].squeeze(), cmap='gray')\n","        else:\n","            # Afficher l'image corrompue dans la deuxième colonne\n","            axs[i, 1].imshow(corrupted_images[i].squeeze())\n","        axs[i, 1].set_title('Image Corrompue')  # Titre de la deuxième colonne\n","        axs[i, 1].axis('off')  # Masquer les axes\n","\n","    plt.show()  # Afficher la figure\n","def show_images(autoencoder, images=None):\n","    \"\"\"\n","    Fonction pour afficher les images originales et leurs reconstructions générées par l'autoencodeur.\n","\n","    Si aucun lot d'images n'est fourni, la fonction récupère un lot d'images depuis le générateur de données.\n","\n","    Arguments :\n","    - autoencoder : L'autoencodeur entraîné, utilisé pour générer des reconstructions des images.\n","    - images (optionnel) : Un tableau d'images à afficher. Si non spécifié, un lot d'images est récupéré du générateur de données.\n","\n","    Sortie :\n","    - Affichage des images originales et leurs reconstructions générées par l'autoencodeur.\n","    \"\"\"\n","\n","    if images is None:\n","        example_batch = next(data_flow)\n","        example_batch = example_batch[0]\n","        images = example_batch[:10]\n","\n","    n_to_show = images.shape[0]\n","\n","    # Génération des reconstructions des images\n","    reconst_images = autoencoder.predict(images)\n","\n","    # Création de la figure pour l'affichage\n","    fig = plt.figure(figsize=(15, 3))\n","    fig.subplots_adjust(hspace=0.4, wspace=0.4)\n","\n","    # Affichage des images originales\n","    for i in range(n_to_show):\n","        img = images[i].squeeze()\n","        sub = fig.add_subplot(2, n_to_show, i+1)\n","        sub.axis('off')\n","        sub.imshow(img)\n","\n","    # Affichage des images reconstruites\n","    for i in range(n_to_show):\n","        img = reconst_images[i].squeeze()\n","        sub = fig.add_subplot(2, n_to_show, i+n_to_show+1)\n","        sub.axis('off')\n","        sub.imshow(img)\n"],"metadata":{"id":"pvPXwLYAEGHw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["INPUT_DIM = (128, 128, 3)   # Dimensions des images (hauteur, largeur, canaux)\n","BATCH_SIZE = 32            # Taille des lots\n","LATENT_DIM = 200            # Dimension du vecteur latent pour l'autoencodeur\n","# Z_DIM définit la dimension de l'espace latent\n","Z_DIM = 200  # Dimension de l'espace latent\n","\n","# Génération des données par lot avec normalisation des pixels (valeurs entre 0 et 1)\n","data_flow = ImageDataGenerator(rescale=1./255).flow_from_directory(\n","    directory=DATA_FOLDER,\n","    classes=['fox'],\n","    target_size=INPUT_DIM[:2],  # Redimensionne les images en 128x128\n","    batch_size=BATCH_SIZE,\n","    shuffle=True,\n","    seed=42,\n","    class_mode='input'  # Dans un autoencodeur, les entrées et sorties sont identiques\n",")\n","\n","# Obtention du premier batch d'images générées\n","data_batch = next(data_flow)\n","\n","# Affichage des 30 premières images du batch\n","import matplotlib.pyplot as plt\n","\n","fig, axes = plt.subplots(3, 10, figsize=(12, 6))\n","for i, ax in enumerate(axes.flatten()):\n","    ax.imshow(data_batch[0][i])\n","    ax.axis('off')\n","plt.show()\n","print(f\"Taille du batch généré : {data_batch[0].shape}\")"],"metadata":{"id":"7haIsdcDDE2m"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","# ENCODER\n","def build_encoder(input_dim, output_dim):\n","    # En utilisant clear_session, nous supprimons toutes les exécutions précédentes. Cela permet de réinitialiser le modèle.\n","    tf.keras.backend.clear_session()  # Utilisation de tensorflow.keras.backend pour clear_session\n","\n","    # Définition de l'entrée du modèle\n","    encoder_input = Input(shape=input_dim, name='encoder_input')\n","    x = encoder_input\n","\n","    # Première convolution\n","    x = Conv2D(filters=32,\n","               kernel_size=3,\n","               strides=2,\n","               padding='same', name='encoder_conv_1')(x)\n","    # Application de la fonction d'activation\n","    x = LeakyReLU()(x)\n","\n","    # Deuxième convolution\n","    x = Conv2D(filters=64,\n","               kernel_size=3,\n","               strides=2,\n","               padding='same', name='encoder_conv_2')(x)\n","    # Application de la fonction d'activation\n","    x = LeakyReLU()(x)\n","\n","    # Troisième convolution\n","    x = Conv2D(filters=64,\n","               kernel_size=3,\n","               strides=2,\n","               padding='same', name='encoder_conv_3')(x)\n","    # Application de la fonction d'activation\n","    x = LeakyReLU()(x)\n","\n","    # Quatrième convolution\n","    x = Conv2D(filters=64,\n","               kernel_size=3,\n","               strides=2,\n","               padding='same', name='encoder_conv_4')(x)\n","    # Application de la fonction d'activation\n","    x = LeakyReLU()(x)\n","\n","    # Nous devons obtenir la forme de la sortie avant de l'aplatir pour l'envoyer au décodeur\n","    shape_before_flattening = tf.keras.backend.int_shape(x)[1:]  # Correction ici : tf.keras.backend.int_shape\n","\n","    # Aplatissement de la dernière couche CNN\n","    x = Flatten()(x)\n","\n","    # Définition de la sortie du modèle\n","    encoder_output = Dense(output_dim, name='encoder_output')(x)\n","\n","    # Retourne l'entrée, la sortie, la forme avant aplatissement et le modèle complet\n","    return encoder_input, encoder_output, shape_before_flattening, Model(encoder_input, encoder_output)\n","\n","    # DECODER\n","\n","def build_decoder(input_dim, shape_before_flattening):\n","  # Définir l'entrée du modèle\n","  decoder_input = Input(shape = (input_dim,) , name = 'decoder_input')\n","\n","  # Pour obtenir une image miroir exacte de l'encodeur\n","  x = Dense(np.prod(shape_before_flattening))(decoder_input)  # Convertir le vecteur en une matrice\n","  x = Reshape(shape_before_flattening)(x)  # Redimensionner en la forme d'avant le Flatten\n","\n","  # Première couche Conv2DTranspose\n","  x = Conv2DTranspose(filters = 64,\n","                  kernel_size = 3,\n","                  strides = 2,\n","                  padding = 'same',\n","                  name = 'decoder_conv_1'\n","                  )(x)\n","  # Application de la fonction d'activation\n","  x = LeakyReLU()(x)\n","\n","  # Deuxième couche Conv2DTranspose\n","  x = Conv2DTranspose(filters = 64,\n","                  kernel_size = 3,\n","                  strides = 2,\n","                  padding = 'same',\n","                  name = 'decoder_conv_2'\n","                  )(x)\n","  # Application de la fonction d'activation\n","  x = LeakyReLU()(x)\n","\n","  # Troisième couche Conv2DTranspose\n","  x = Conv2DTranspose(filters = 32,\n","                  kernel_size = 3,\n","                  strides = 2,\n","                  padding = 'same',\n","                  name = 'decoder_conv_3'\n","                  )(x)\n","  # Application de la fonction d'activation\n","  x = LeakyReLU()(x)\n","\n","  # Quatrième couche Conv2DTranspose\n","  x = Conv2DTranspose(filters = 3,\n","                  kernel_size = 3,\n","                  strides = 2,\n","                  padding = 'same',\n","                  name = 'decoder_conv_4'\n","                  )(x)\n","  # Application de la fonction d'activation\n","  x = Activation('sigmoid')(x)  # Activation finale pour obtenir une sortie entre 0 et 1\n","\n","  # Sortie du décodeur\n","  decoder_output = x\n","\n","  return decoder_input, decoder_output, Model(decoder_input, decoder_output)"],"metadata":{"id":"teF3-8YfDaNj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","# Création de l'encodeur avec les dimensions d'entrée et de sortie spécifiées\n","encoder_input, encoder_output, shape_before_flattening, encoder  = build_encoder(input_dim = INPUT_DIM,\n","                                    output_dim = Z_DIM,\n","                                    )\n","\n","encoder.summary()\n"],"metadata":{"id":"cOJdtDOxDNCC","collapsed":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Création du décodeur en utilisant la dimension de l'espace latent et la forme avant le flattening\n","decoder_input, decoder_output, decoder = build_decoder(input_dim = Z_DIM,\n","                                        shape_before_flattening = shape_before_flattening\n","                                        )\n","\n","decoder.summary()"],"metadata":{"id":"FnSD0uuvDhyt","collapsed":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# L'entrée du modèle sera l'image envoyée à l'encodeur.\n","autoencoder_input = encoder_input\n","\n","# La sortie sera celle du décodeur. Le terme - decoder(encoder_output)\n","# permet de combiner les deux modèles en envoyant la sortie de l'encodeur comme entrée du décodeur.\n","autoencoder_output = decoder(encoder_output)\n","\n","# L'entrée du modèle combiné sera donc l'entrée de l'encodeur.\n","# La sortie du modèle combiné sera la sortie du décodeur.\n","autoencoder = Model(autoencoder_input, autoencoder_output)\n","\n","\n","autoencoder.summary()"],"metadata":{"id":"ZPUg4ENUDsGG","collapsed":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# parameters of the model\n","LEARNING_RATE=.0005\n","steps_per_epoch=int(NUM_IMAGES / BATCH_SIZE)\n","epochs=0\n","initial_epoch=0"],"metadata":{"id":"B9TYgLoGDuMv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(\"vos fichiers disponibles : \")\n","!ls /content/gdrive/MyDrive/Colab\\ Notebooks/ML_FDS/myweights/AE\n","\n","epoch_number = int(input(\"Entrez le numéro de l'époque à charger (ex: 40) (0 si aucun fichier existant) : \"))\n","epochs = int(input(\"Entrez le numéro d'epochs à faire en plus (ex: 25) : \"))\n","filename = f\"AE.{epoch_number:02d}.weights.h5\"\n","filepath = os.path.join(WEIGHTS_FOLDER, filename)\n","\n","# === Construction du modèle (toujours reconstruit, puis poids chargés si besoin) ===\n","\n","# Recréation propre du modèle\n","encoder_input, encoder_output, shape_before_flattening, encoder = build_encoder(INPUT_DIM, Z_DIM)\n","decoder_input, decoder_output, decoder = build_decoder(Z_DIM, shape_before_flattening)\n","\n","autoencoder_input = encoder_input\n","autoencoder_output = decoder(encoder_output)\n","autoencoder = Model(autoencoder_input, autoencoder_output)\n","\n","if epoch_number == 0:\n","    print(\"Nouveau modèle créé (pas de poids chargés)\")\n","else:\n","    if os.path.exists(filepath):\n","        autoencoder.load_weights(filepath)\n","        print(f\"Poids chargés depuis : {filepath}\")\n","    else:\n","        print(f\"Fichier introuvable : {filepath}\")\n","        sys.exit(1)\n","initial_epoch = epoch_number\n","# Compilation de l'autoencodeur avec l'optimiseur Adam et un taux d'apprentissage spécifié,\n","# ainsi que l'utilisation de la perte 'mean_squared_error' et de l'exactitude comme métrique\n","autoencoder.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE), loss='mean_squared_error' , metrics=['accuracy'])\n","\n","# Création des données de validation pour l'affichage\n","\n","val_data_flow=tion_data = next(data_flow)\n","\n","# Définition d'un point de contrôle pour sauvegarder les poids du modèle pendant l'entraînement\n","# Le modèle sauvegardera uniquement les poids dans un fichier .h5\n","checkpoint_autoencoder = ModelCheckpoint(os.path.join(WEIGHTS_FOLDER, 'AE.{epoch:02d}.weights.h5'), save_weights_only = True, verbose=1, save_freq=steps_per_epoch * 25)\n","\n","# Entraînement de l'autoencodeur avec les données d'entraînement,\n","# en utilisant les callbacks pour sauvegarder les poids du modèle à chaque époque\n","history=autoencoder.fit(data_flow,\n","                     shuffle=True,\n","                     epochs = initial_epoch+epochs,\n","                     initial_epoch = initial_epoch,\n","                     steps_per_epoch=steps_per_epoch,\n","                     validation_data=val_data_flow,\n","                     callbacks=[checkpoint_autoencoder])\n"],"metadata":{"id":"fKp0lEmHCowd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(\"vos fichiers disponibles : \")\n","!ls /content/gdrive/MyDrive/Colab\\ Notebooks/ML_FDS/myweights/AE\n","epoch_number = int(input(\"Entrez le numéro de l'époque à charger (ex: 40) : \"))\n","\n","filename = f\"AE.{epoch_number:02d}.weights.h5\"\n","filepath = os.path.join(WEIGHTS_FOLDER, filename)\n","\n","if os.path.exists(filepath):\n","    autoencoder.load_weights(filepath)\n","    print(f\"Poids chargés depuis : {filepath}\")\n","else:\n","    print(f\"Fichier introuvable : {filepath}\")\n","    sys.exit(1)\n","# Récupère le premier lot d'images du générateur de données\n","example_batch = next(data_flow)\n","\n","# Sélectionne uniquement les images du batch (le premier élément contient les images)\n","example_batch = example_batch[0]\n","\n","# Prend les 10 premières images du lot pour les afficher\n","example_images = example_batch[:10]\n","\n","# Affiche les images originales et reconstruites par l'autoencodeur\n","show_images(autoencoder, example_images)"],"metadata":{"id":"Ps478NErD9M6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.manifold import TSNE\n","from matplotlib.offsetbox import OffsetImage, AnnotationBbox\n","from tensorflow.keras.utils import array_to_img\n","def visualize_latent_space_fox(encoder_model, data_flow, num_images=100, image_size=(64, 64), zoom=0.8):\n","    \"\"\"\n","    Affiche les renards dans l’espace latent à partir de l’encodeur.\n","\n","    Paramètres :\n","    ------------\n","    encoder_model : keras.Model\n","        Le modèle encodeur (extrait du modèle autoencodeur entraîné).\n","    data_flow : ImageDataGenerator.flow_from_directory\n","        Le générateur de données (ici contenant uniquement des images de renards).\n","    num_images : int\n","        Nombre d’images à afficher.\n","    image_size : tuple\n","        Taille des vignettes.\n","    zoom : float\n","        Zoom appliqué à chaque vignette.\n","    \"\"\"\n","    # Charger toutes les images nécessaires (en 1 ou plusieurs batches)\n","    X_images = []\n","    loaded = 0\n","    while loaded < num_images:\n","        batch = next(data_flow)[0]\n","        remaining = num_images - loaded\n","        if batch.shape[0] > remaining:\n","            X_images.extend(batch[:remaining])\n","            break\n","        else:\n","            X_images.extend(batch)\n","            loaded += batch.shape[0]\n","    X_images = np.array(X_images)\n","\n","    # Encodage avec le modèle\n","    latent_vectors = encoder_model.predict(X_images, verbose=0)\n","    if latent_vectors.ndim > 2:\n","        latent_vectors = latent_vectors.reshape((latent_vectors.shape[0], -1))\n","\n","    # Réduction de dimension via t-SNE\n","    tsne = TSNE(n_components=2, random_state=42)\n","    latent_2d = tsne.fit_transform(latent_vectors)\n","\n","    # Affichage\n","    plt.figure(figsize=(15, 12))\n","    ax = plt.gca()\n","    for i, (x, y) in enumerate(latent_2d):\n","        thumbnail = array_to_img(X_images[i])\n","        thumbnail = thumbnail.resize(image_size)\n","        imagebox = OffsetImage(thumbnail, zoom=zoom)\n","        ab = AnnotationBbox(imagebox, (x, y), frameon=False)\n","        ax.add_artist(ab)\n","\n","    ax.set_xlim(latent_2d[:, 0].min() - 5, latent_2d[:, 0].max() + 5)\n","    ax.set_ylim(latent_2d[:, 1].min() - 5, latent_2d[:, 1].max() + 5)\n","    plt.title(\"Répartition des images de renards dans l’espace latent\")\n","    plt.xlabel(\"Dimension 1\")\n","    plt.ylabel(\"Dimension 2\")\n","    ax.grid(False)\n","    plt.show()\n","\n","visualize_latent_space_fox(encoder_model=encoder, data_flow=data_flow, num_images=100)\n"],"metadata":{"id":"sNtQXAPKk-kt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Noising"],"metadata":{"id":"a2-OOLfsEFMN"}},{"cell_type":"code","source":["BATCH_SIZE = 12\n","INPUT_DIM = (128,128,3)\n","classes=['fox']\n","target_size=INPUT_DIM[:2]\n","typeimage='noise'\n","# Create the generator\n","mydataflow=my_image_generator(folder=DATA_FOLDER, classes=classes, target_size=target_size,batch_size=BATCH_SIZE, typeimage=typeimage)\n","\n","# Get a batch\n","batch = next(mydataflow)\n","display_images_from_batch(batch,typeimage )"],"metadata":{"id":"ZlcP3Kd9MaVd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(\"vos fichiers disponibles : \")\n","!ls /content/gdrive/MyDrive/Colab\\ Notebooks/ML_FDS/myweights/AENoise\n","\n","epoch_number = int(input(\"Entrez le numéro de l'époque à charger (ex: 40) (0 si aucun fichier existant) : \"))\n","epochs = int(input(\"Entrez le numéro d'epochs à faire en plus (ex: 25) : \"))\n","filename = f\"AENoise.{epoch_number:02d}.weights.h5\"\n","filepath = os.path.join(WEIGHTS_FOLDER2, filename)\n","\n","# === Construction du modèle (toujours reconstruit, puis poids chargés si besoin) ===\n","\n","# Recréation propre du modèle\n","encoder_input, encoder_output, shape_before_flattening, encoder = build_encoder(INPUT_DIM, Z_DIM)\n","decoder_input, decoder_output, decoder = build_decoder(Z_DIM, shape_before_flattening)\n","\n","autoencoder_input = encoder_input\n","autoencoder_output = decoder(encoder_output)\n","autoencoder = Model(autoencoder_input, autoencoder_output)\n","\n","if epoch_number == 0:\n","    print(\"Nouveau modèle créé (pas de poids chargés)\")\n","else:\n","    if os.path.exists(filepath):\n","        autoencoder.load_weights(filepath)\n","        print(f\"Poids chargés depuis : {filepath}\")\n","    else:\n","        print(f\"Fichier introuvable : {filepath}\")\n","        sys.exit(1)\n","initial_epoch = epoch_number\n","# Compilation du modèle autoencodeur avec un optimiseur Adam et une fonction de perte 'mean_squared_error'\n","# L'optimiseur utilise un taux d'apprentissage défini par la variable LEARNING_RATE\n","# Le modèle sera évalué avec l'accuracy comme métrique\n","autoencoder.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE), loss='mean_squared_error', metrics=['accuracy'])\n","\n","# Création des données de validation pour l'affichage\n","# Récupération du premier batch du générateur de données\n","val_data_flow = next(mydataflow)\n","\n","# Définition d'un callback de sauvegarde des poids du modèle\n","# Les poids seront sauvegardés sous le nom 'AEweights_corrupted.h5' dans le dossier spécifié par WEIGHTS_FOLDER\n","# Les poids sont sauvegardés à chaque époque et le mode verbose est activé pour afficher des informations pendant l'entraînement\n","checkpoint_autoencoder = ModelCheckpoint(os.path.join(WEIGHTS_FOLDER2, 'AENoise.{epoch:02d}.weights.h5'), save_weights_only=True, verbose=1,save_freq=steps_per_epoch * 25)\n","\n","# Entraînement du modèle autoencodeur\n","\n","history = autoencoder.fit(mydataflow,\n","                          shuffle=True,\n","                          epochs = initial_epoch+epochs,\n","                          initial_epoch = initial_epoch,\n","                          steps_per_epoch=steps_per_epoch,\n","                          validation_data=val_data_flow,\n","                          callbacks=[checkpoint_autoencoder])"],"metadata":{"id":"4VfXkN7HNRCk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(\"vos fichiers disponibles : \")\n","!ls /content/gdrive/MyDrive/Colab\\ Notebooks/ML_FDS/myweights/AENoise\n","epoch_number = int(input(\"Entrez le numéro de l'époque à charger (ex: 40) : \"))\n","\n","filename = f\"AENoise.{epoch_number:02d}.weights.h5\"\n","filepath = os.path.join(WEIGHTS_FOLDER2, filename)\n","\n","if os.path.exists(filepath):\n","    autoencoder.load_weights(filepath)\n","    print(f\"Poids chargés depuis : {filepath}\")\n","else:\n","    print(f\"Fichier introuvable : {filepath}\")\n","    sys.exit(1)\n","\n","example_batch = next(mydataflow)\n","example_batch = example_batch[0]\n","example_images = example_batch[:10]\n","show_images(autoencoder,example_images)"],"metadata":{"id":"8oUwLJT_P4SK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Noir et blanc"],"metadata":{"id":"MIZHEly3RgRg"}},{"cell_type":"code","source":["BATCH_SIZE = 12\n","INPUT_DIM = (128,128,3)\n","classes=['fox']\n","target_size=INPUT_DIM[:2]\n","typeimage='blackwhite'\n","# Create the generator\n","mydataflow=my_image_generator(folder=DATA_FOLDER, classes=classes, target_size=target_size,batch_size=BATCH_SIZE, typeimage=typeimage)\n","\n","# Get a batch\n","batch = next(mydataflow)\n","display_images_from_batch(batch,typeimage)\n","\n"],"metadata":{"id":"xyCs9DjpRh3m"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(\"vos fichiers disponibles : \")\n","!ls /content/gdrive/MyDrive/Colab\\ Notebooks/ML_FDS/myweights/AENB\n","\n","epoch_number = int(input(\"Entrez le numéro de l'époque à charger (ex: 40) (0 si aucun fichier existant) : \"))\n","epochs = int(input(\"Entrez le numéro d'epochs à faire en plus (ex: 25) : \"))\n","filename = f\"AENB.{epoch_number:02d}.weights.h5\"\n","filepath = os.path.join(WEIGHTS_FOLDER3, filename)\n","\n","# === Construction du modèle (toujours reconstruit, puis poids chargés si besoin) ===\n","\n","# Recréation propre du modèle\n","encoder_input, encoder_output, shape_before_flattening, encoder = build_encoder(INPUT_DIM, Z_DIM)\n","decoder_input, decoder_output, decoder = build_decoder(Z_DIM, shape_before_flattening)\n","\n","autoencoder_input = encoder_input\n","autoencoder_output = decoder(encoder_output)\n","autoencoder = Model(autoencoder_input, autoencoder_output)\n","\n","if epoch_number == 0:\n","    print(\"Nouveau modèle créé (pas de poids chargés)\")\n","else:\n","    if os.path.exists(filepath):\n","        autoencoder.load_weights(filepath)\n","        print(f\"Poids chargés depuis : {filepath}\")\n","    else:\n","        print(f\"Fichier introuvable : {filepath}\")\n","        sys.exit(1)\n","initial_epoch = epoch_number\n","\n","# Compilation du modèle autoencodeur avec un optimiseur Adam et une fonction de perte 'mean_squared_error'\n","# L'optimiseur utilise un taux d'apprentissage défini par la variable LEARNING_RATE\n","# Le modèle sera évalué avec l'accuracy comme métrique\n","autoencoder.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE), loss='mean_squared_error', metrics=['accuracy'])\n","\n","# Création des données de validation pour l'affichage\n","# Récupération du premier batch du générateur de données\n","val_data_flow = next(mydataflow)\n","\n","# Définition d'un callback de sauvegarde des poids du modèle\n","# Les poids seront sauvegardés sous le nom 'AEweights_corrupted.h5' dans le dossier spécifié par WEIGHTS_FOLDER\n","# Les poids sont sauvegardés à chaque époque et le mode verbose est activé pour afficher des informations pendant l'entraînement\n","checkpoint_autoencoder = ModelCheckpoint(os.path.join(WEIGHTS_FOLDER3, 'AENB.{epoch:02d}.weights.h5'), save_weights_only=True, verbose=1,save_freq=steps_per_epoch * 25)\n","\n","# Entraînement du modèle autoencodeur\n","\n","history = autoencoder.fit(mydataflow,\n","                          shuffle=True,\n","                          epochs = initial_epoch+epochs,\n","                          initial_epoch = initial_epoch,\n","                          steps_per_epoch=steps_per_epoch,\n","                          validation_data=val_data_flow,\n","                          callbacks=[checkpoint_autoencoder])"],"metadata":{"id":"-OWm-sgkRlFl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(\"vos fichiers disponibles : \")\n","!ls /content/gdrive/MyDrive/Colab\\ Notebooks/ML_FDS/myweights/AENB\n","epoch_number = int(input(\"Entrez le numéro de l'époque à charger (ex: 40) : \"))\n","\n","filename = f\"AENB.{epoch_number:02d}.weights.h5\"\n","filepath = os.path.join(WEIGHTS_FOLDER3, filename)\n","\n","if os.path.exists(filepath):\n","    autoencoder.load_weights(filepath)\n","    print(f\"Poids chargés depuis : {filepath}\")\n","else:\n","    print(f\"Fichier introuvable : {filepath}\")\n","    sys.exit(1)\n","\n","example_batch = next(mydataflow)\n","example_batch = example_batch[0]\n","example_images = example_batch[:10]\n","show_images(autoencoder,example_images)"],"metadata":{"id":"kKPG4GK8Rt2B"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Create a separate encoder\n","input_encoder, output_encoder,  _, encoder_model  = build_encoder(input_dim = INPUT_DIM,\n","                                    output_dim = Z_DIM,\n","                                    )\n","#encoder_input, encoder_output, _, encoder_model = build_encoder(input_dim, output_dim)\n","# Create a batch as X_test\n","X_test = next(mydataflow)\n","# Get the corrupted image\n","X_test = X_test[0]\n","\n","# Get the latent space\n","latent_codes = encoder_model.predict(X_test)\n","\n","# Use PCA to get the 2 main components\n","pca = PCA(n_components=2)\n","latent_codes_2d = pca.fit_transform(latent_codes)\n","\n","# Plot the latent space\n","plt.scatter(latent_codes_2d[:, 0], latent_codes_2d[:, 1])\n","plt.xlabel('Latent Dimension 1')\n","plt.ylabel('Latent Dimension 2')\n","plt.title('Latent Space')\n","plt.show()\n","latent_codes_2d = pca.fit_transform(latent_codes)"],"metadata":{"id":"kk7kDhMgzTaK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_vector = np.random.normal(size=(1, Z_DIM))\n","generated_image = decoder.predict(test_vector)\n","\n","plt.imshow(generated_image[0])\n","plt.show()"],"metadata":{"id":"IUzhYN074F5t"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["FAUSSES IMAGES"],"metadata":{"id":"qaX2-NJUc40t"}},{"cell_type":"code","source":["INPUT_DIM = (128, 128, 3)   # Dimensions des images (hauteur, largeur, canaux)\n","BATCH_SIZE = 32            # Taille des lots\n","LATENT_DIM = 200            # Dimension du vecteur latent pour l'autoencodeur\n","# Z_DIM définit la dimension de l'espace latent\n","Z_DIM = 200  # Dimension de l'espace latent\n","\n","# Génération des données par lot avec normalisation des pixels (valeurs entre 0 et 1)\n","data_flow = ImageDataGenerator(rescale=1./255).flow_from_directory(\n","    directory=DATA_FOLDER,\n","    classes=['elephant'],\n","    target_size=INPUT_DIM[:2],  # Redimensionne les images en 128x128\n","    batch_size=BATCH_SIZE,\n","    shuffle=True,\n","    seed=42,\n","    class_mode='input'  # Dans un autoencodeur, les entrées et sorties sont identiques\n",")\n","\n","# Obtention du premier batch d'images générées\n","data_batch = next(data_flow)\n","\n","# Affichage des 30 premières images du batch\n","import matplotlib.pyplot as plt\n","\n","fig, axes = plt.subplots(3, 10, figsize=(12, 6))\n","for i, ax in enumerate(axes.flatten()):\n","    ax.imshow(data_batch[0][i])\n","    ax.axis('off')\n","plt.show()\n","print(f\"Taille du batch généré : {data_batch[0].shape}\")"],"metadata":{"id":"fnOXAs1Vc7IL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(\"vos fichiers disponibles : \")\n","!ls /content/gdrive/MyDrive/Colab\\ Notebooks/ML_FDS/myweights/AEElephant\n","\n","epoch_number = int(input(\"Entrez le numéro de l'époque à charger (ex: 40) (0 si aucun fichier existant) : \"))\n","epochs = int(input(\"Entrez le numéro d'epochs à faire en plus (ex: 25) : \"))\n","filename = f\"AEElephant.{epoch_number:02d}.weights.h5\"\n","filepath = os.path.join(WEIGHTS_FOLDER4, filename)\n","\n","# === Construction du modèle (toujours reconstruit, puis poids chargés si besoin) ===\n","\n","# Recréation propre du modèle\n","encoder_input, encoder_output, shape_before_flattening, encoder = build_encoder(INPUT_DIM, Z_DIM)\n","decoder_input, decoder_output, decoder = build_decoder(Z_DIM, shape_before_flattening)\n","\n","autoencoder_input = encoder_input\n","autoencoder_output = decoder(encoder_output)\n","autoencoder = Model(autoencoder_input, autoencoder_output)\n","\n","if epoch_number == 0:\n","    print(\"Nouveau modèle créé (pas de poids chargés)\")\n","else:\n","    if os.path.exists(filepath):\n","        autoencoder.load_weights(filepath)\n","        print(f\"Poids chargés depuis : {filepath}\")\n","    else:\n","        print(f\"Fichier introuvable : {filepath}\")\n","        sys.exit(1)\n","initial_epoch = epoch_number\n","# Compilation de l'autoencodeur avec l'optimiseur Adam et un taux d'apprentissage spécifié,\n","# ainsi que l'utilisation de la perte 'mean_squared_error' et de l'exactitude comme métrique\n","autoencoder.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE), loss='mean_squared_error' , metrics=['accuracy'])\n","\n","# Création des données de validation pour l'affichage\n","\n","val_data_flow=tion_data = next(data_flow)\n","\n","# Définition d'un point de contrôle pour sauvegarder les poids du modèle pendant l'entraînement\n","# Le modèle sauvegardera uniquement les poids dans un fichier .h5\n","checkpoint_autoencoder = ModelCheckpoint(os.path.join(WEIGHTS_FOLDER4, 'AEElephant.{epoch:02d}.weights.h5'), save_weights_only = True, verbose=1, save_freq=steps_per_epoch * 25)\n","\n","# Entraînement de l'autoencodeur avec les données d'entraînement,\n","# en utilisant les callbacks pour sauvegarder les poids du modèle à chaque époque\n","history=autoencoder.fit(data_flow,\n","                     shuffle=True,\n","                     epochs = initial_epoch+epochs,\n","                     initial_epoch = initial_epoch,\n","                     steps_per_epoch=steps_per_epoch,\n","                     validation_data=val_data_flow,\n","                     callbacks=[checkpoint_autoencoder])\n"],"metadata":{"id":"ucsD0p63figJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["INPUT_DIM = (128, 128, 3)   # Dimensions des images (hauteur, largeur, canaux)\n","BATCH_SIZE = 32            # Taille des lots\n","LATENT_DIM = 200            # Dimension du vecteur latent pour l'autoencodeur\n","# Z_DIM définit la dimension de l'espace latent\n","Z_DIM = 200  # Dimension de l'espace latent\n","\n","# Génération des données par lot avec normalisation des pixels (valeurs entre 0 et 1)\n","data_flow = ImageDataGenerator(rescale=1./255).flow_from_directory(\n","    directory=DATA_FOLDER,\n","    classes=['tiger'],\n","    target_size=INPUT_DIM[:2],  # Redimensionne les images en 128x128\n","    batch_size=BATCH_SIZE,\n","    shuffle=True,\n","    seed=42,\n","    class_mode='input'  # Dans un autoencodeur, les entrées et sorties sont identiques\n",")\n","\n","# Obtention du premier batch d'images générées\n","data_batch = next(data_flow)\n","\n","# Affichage des 30 premières images du batch\n","import matplotlib.pyplot as plt\n","\n","fig, axes = plt.subplots(3, 10, figsize=(12, 6))\n","for i, ax in enumerate(axes.flatten()):\n","    ax.imshow(data_batch[0][i])\n","    ax.axis('off')\n","plt.show()\n","print(f\"Taille du batch généré : {data_batch[0].shape}\")"],"metadata":{"id":"5mbA0W38nMXA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(\"vos fichiers disponibles : \")\n","!ls /content/gdrive/MyDrive/Colab\\ Notebooks/ML_FDS/myweights/AETiger\n","\n","epoch_number = int(input(\"Entrez le numéro de l'époque à charger (ex: 40) (0 si aucun fichier existant) : \"))\n","epochs = int(input(\"Entrez le numéro d'epochs à faire en plus (ex: 25) : \"))\n","filename = f\"AETiger.{epoch_number:02d}.weights.h5\"\n","filepath = os.path.join(WEIGHTS_FOLDER5, filename)\n","\n","# === Construction du modèle (toujours reconstruit, puis poids chargés si besoin) ===\n","\n","# Recréation propre du modèle\n","encoder_input, encoder_output, shape_before_flattening, encoder = build_encoder(INPUT_DIM, Z_DIM)\n","decoder_input, decoder_output, decoder = build_decoder(Z_DIM, shape_before_flattening)\n","\n","autoencoder_input = encoder_input\n","autoencoder_output = decoder(encoder_output)\n","autoencoder = Model(autoencoder_input, autoencoder_output)\n","\n","if epoch_number == 0:\n","    print(\"Nouveau modèle créé (pas de poids chargés)\")\n","else:\n","    if os.path.exists(filepath):\n","        autoencoder.load_weights(filepath)\n","        print(f\"Poids chargés depuis : {filepath}\")\n","    else:\n","        print(f\"Fichier introuvable : {filepath}\")\n","        sys.exit(1)\n","initial_epoch = epoch_number\n","# Compilation de l'autoencodeur avec l'optimiseur Adam et un taux d'apprentissage spécifié,\n","# ainsi que l'utilisation de la perte 'mean_squared_error' et de l'exactitude comme métrique\n","autoencoder.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE), loss='mean_squared_error' , metrics=['accuracy'])\n","\n","# Création des données de validation pour l'affichage\n","\n","val_data_flow=tion_data = next(data_flow)\n","\n","# Définition d'un point de contrôle pour sauvegarder les poids du modèle pendant l'entraînement\n","# Le modèle sauvegardera uniquement les poids dans un fichier .h5\n","checkpoint_autoencoder = ModelCheckpoint(os.path.join(WEIGHTS_FOLDER5, 'AETiger.{epoch:02d}.weights.h5'), save_weights_only = True, verbose=1, save_freq=steps_per_epoch * 25)\n","\n","# Entraînement de l'autoencodeur avec les données d'entraînement,\n","# en utilisant les callbacks pour sauvegarder les poids du modèle à chaque époque\n","history=autoencoder.fit(data_flow,\n","                     shuffle=True,\n","                     epochs = initial_epoch+epochs,\n","                     initial_epoch = initial_epoch,\n","                     steps_per_epoch=steps_per_epoch,\n","                     validation_data=val_data_flow,\n","                     callbacks=[checkpoint_autoencoder])\n"],"metadata":{"id":"Imk5EAuBnQeP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","INPUT_DIM = (128,128,3)\n","Z_DIM = 200\n","N_SAMPLES = 5\n","\n","FOX_DIR = '/content/gdrive/MyDrive/Colab Notebooks/ML_FDS/Data_Project/Tiger-Fox-Elephant/fox'\n","TIGER_DIR = '/content/gdrive/MyDrive/Colab Notebooks/ML_FDS/Data_Project/Tiger-Fox-Elephant/tiger'\n","\n","WEIGHTS_FOX = '/content/gdrive/MyDrive/Colab Notebooks/ML_FDS/myweights/AE'\n","WEIGHTS_NEG = '/content/gdrive/MyDrive/Colab Notebooks/ML_FDS/myweights/AETiger'\n","\n","def load_n_images(folder, n, target_size):\n","    files = sorted(os.listdir(folder))[:n]\n","    imgs = []\n","    for f in files:\n","        img = load_img(os.path.join(folder, f), target_size=target_size)\n","        arr = img_to_array(img) / 255.\n","        imgs.append(arr)\n","    return np.stack(imgs, axis=0)\n","\n","enc_in_f, enc_out_f, shp_f, encoder_fox   = build_encoder(INPUT_DIM, Z_DIM)\n","dec_in_f, dec_out_f, decoder_fox         = build_decoder(Z_DIM, shp_f)\n","auto_fox = Model(enc_in_f, decoder_fox(enc_out_f))\n","\n","ep = int(input(\"Époque AE fox (0 = none) : \"))\n","if ep>0:\n","    auto_fox.load_weights(os.path.join(WEIGHTS_FOX, f\"AE.{ep:02d}.weights.h5\"))\n","    print(\"→ fox weights chargés\")\n","\n","enc_in_n, enc_out_n, shp_n, encoder_neg   = build_encoder(INPUT_DIM, Z_DIM)\n","dec_in_n, dec_out_n, decoder_neg         = build_decoder(Z_DIM, shp_n)\n","auto_neg = Model(enc_in_n, decoder_neg(enc_out_n))\n","\n","ep2 = int(input(\"Époque AE negative (0 = none) : \"))\n","if ep2>0:\n","    auto_neg.load_weights(os.path.join(WEIGHTS_NEG, f\"AETiger.{ep2:02d}.weights.h5\"))\n","    print(\"→ negative weights chargés\")\n","\n","X_fox = load_n_images(FOX_DIR, N_SAMPLES, INPUT_DIM[:2])\n","X_neg = load_n_images(TIGER_DIR, N_SAMPLES, INPUT_DIM[:2])\n","\n","z_fox = encoder_fox.predict(X_fox)\n","z_neg = encoder_neg.predict(X_neg)\n","\n","alpha    = 0.5\n","z_fused  = alpha * z_fox + (1-alpha) * z_neg\n","\n","X_fused = decoder_fox.predict(z_fused)\n","\n","fig, axs = plt.subplots(3, N_SAMPLES, figsize=(15,9), dpi=100)\n","for i in range(N_SAMPLES):\n","    axs[0,i].imshow(X_fox[i]);      axs[0,i].axis('off')\n","    axs[1,i].imshow(X_neg[i]);      axs[1,i].axis('off')\n","    axs[2,i].imshow(X_fused[i]);    axs[2,i].axis('off')\n","\n","axs[0,0].set_title(\"Fox originals\")\n","axs[1,0].set_title(\"Tiger originals\")\n","axs[2,0].set_title(\"Fusion (50/50)\")\n","plt.tight_layout()\n","plt.show()\n"],"metadata":{"id":"oP9W_QzHh1Cg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def generate_images_from_noise(n_to_show=10, Z_DIM=64, decoder=None):\n","    \"\"\"\n","    Génère des images à partir de bruit aléatoire en échantillonnant une distribution normale\n","    et en utilisant le décodeur d'un modèle pour recréer des images correspondantes.\n","\n","    Paramètres :\n","    n_to_show (int, optional): Le nombre d'images à générer et afficher. Défaut à 10.\n","    Z_DIM (int, optional): La dimension du vecteur de bruit utilisé pour l'échantillonnage. Défaut à 200.\n","    decoder (model, optional): Le modèle décodeur utilisé pour générer les images à partir du bruit. Il doit être passé en paramètre.\n","    \"\"\"\n","\n","    # Vérification que le modèle décodeur est passé en paramètre\n","    if decoder is None:\n","        raise ValueError(\"Le modèle décodeur n'a pas été fourni. Veuillez passer le modèle décodeur dans le paramètre 'decoder'.\")\n","\n","    # Pour n_to_show images, créer un échantillon suivant une distribution normale\n","    normal_distribution_sample = np.random.normal(0, 1, size=(n_to_show, Z_DIM))\n","\n","    # Appliquer la partie décodeur pour générer les images à partir de l'échantillon\n","    reconst_images = decoder.predict(normal_distribution_sample)\n","\n","    # Créer une figure pour afficher les images générées\n","    fig = plt.figure(figsize=(15, 3))\n","    fig.subplots_adjust(hspace=0.4, wspace=0.4)\n","\n","    # Afficher les images générées\n","    for i in range(n_to_show):\n","        img = reconst_images[i].squeeze()  # Supprimer la dimension de couleur (s'il y en a)\n","        sub = fig.add_subplot(2, n_to_show, i + 1)  # Créer un sous-graphe pour chaque image\n","        sub.axis('off')  # Désactiver les axes pour une meilleure lisibilité\n","        sub.imshow(img)  # Afficher l'image générée\n","\n","    # Afficher la figure finale\n","    plt.show()"],"metadata":{"id":"JHxRW5KG4Jjp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["generate_images_from_noise(decoder=decoder)"],"metadata":{"id":"74NJL92g4NnW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# VAE Encoder\n","def build_vae_encoder(input_dim, output_dim):\n","    \"\"\"\n","    Construit l'encodeur d'un Variational Autoencoder (VAE), qui réduit les images en un espace latent de dimension `output_dim`.\n","\n","    Paramètres :\n","    -----------\n","    input_dim : tuple\n","        Dimension d'entrée des images (hauteur, largeur, canaux).\n","    output_dim : int\n","        Dimension de l'espace latent, ou taille du vecteur latent généré par l'encodeur.\n","\n","    Retourne :\n","    --------\n","    tuple:\n","        - vae_encoder_input (Input): Le tensor d'entrée pour l'encodeur.\n","        - vae_encoder_output (Tensor): Le tensor de sortie de l'encodeur après échantillonnage.\n","        - mean_mu (Tensor): La moyenne de la distribution de l'espace latent.\n","        - log_var (Tensor): Le logarithme de la variance de l'espace latent.\n","        - shape_before_flattening (tuple): La forme du tensor avant le Flatten, utilisée pour le décodeur.\n","        - Model: Le modèle Keras complet de l'encodeur VAE.\n","\n","    Structure du réseau:\n","    ---------------------\n","    1. Couches de convolution pour réduire les dimensions spatiales de l'image tout en augmentant les caractéristiques.\n","    2. Calcul des statistiques `mean_mu` et `log_var` pour modéliser la distribution de l'espace latent.\n","    3. La fonction d'échantillonnage applique le \"reparameterization trick\" pour obtenir un vecteur latent différentiable.\n","    4. Calcul de la perte KL Divergence pour imposer une régularisation sur l'espace latent.\n","    \"\"\"\n","\n","    # Réinitialiser les sessions précédentes de Keras (utile dans un environnement Jupyter)\n","    tf.keras.backend.clear_session()\n","\n","    # Définir l'entrée du modèle encodeur, prenant des images de dimension `input_dim`\n","    vae_encoder_input = Input(shape=input_dim, name='vae_encoder_input')\n","    x = vae_encoder_input\n","\n","    # Première couche de convolution pour extraire les caractéristiques initiales\n","    x = Conv2D(32, 3, strides=2, padding='same', name='vae_encoder_conv_1')(x)\n","    x = LeakyReLU()(x)\n","\n","    # Deuxième couche de convolution pour réduire les dimensions spatiales et augmenter la profondeur\n","    x = Conv2D(64, 3, strides=2, padding='same', name='vae_encoder_conv_2')(x)\n","    x = LeakyReLU()(x)\n","\n","    # Troisième couche de convolution pour extraire davantage de caractéristiques\n","    x = Conv2D(64, 3, strides=2, padding='same', name='vae_encoder_conv_3')(x)\n","    x = LeakyReLU()(x)\n","\n","    # Quatrième couche de convolution pour une réduction spatiale maximale et une extraction de caractéristiques\n","    x = Conv2D(64, 3, strides=2, padding='same', name='vae_encoder_conv_4')(x)\n","    x = LeakyReLU()(x)\n","\n","    # Capture de la forme avant Flatten, utilisée pour le décodeur\n","    shape_before_flattening = tf.keras.backend.int_shape(x)[1:]\n","    x = Flatten()(x)\n","\n","    # Calcul de la moyenne (mu) et du log de la variance (log_var) pour l'échantillonnage dans l'espace latent\n","    mean_mu = Dense(output_dim, name='mu')(x)\n","    log_var = Dense(output_dim, name='log_var')(x)\n","\n","    # Fonction d'échantillonnage appliquant le \"reparameterization trick\" pour échantillonner de la distribution\n","    def sampling(args):\n","        mean_mu, log_var = args\n","        epsilon = K.random_normal(shape=K.shape(mean_mu), mean=0., stddev=1.)\n","        return mean_mu + tf.exp(log_var / 2) * epsilon  # Retourne l'échantillon latent\n","\n","    # Application du sampling avec une couche Lambda\n","    vae_encoder_output = Lambda(sampling, output_shape=(output_dim,), name='vae_encoder_output')([mean_mu, log_var])\n","\n","    # Couche de calcul de la perte KL Divergence pour imposer une régularisation de l'espace latent\n","    class KLLossLayer(tf.keras.layers.Layer):\n","        def __init__(self, **kwargs):\n","            super(KLLossLayer, self).__init__(**kwargs)\n","\n","        def call(self, inputs):\n","            mean_mu, log_var = inputs\n","            # Calcul de la divergence KL entre la distribution latente et une normale standard\n","            kl_loss = -0.5 * tf.reduce_sum(1 + log_var - tf.square(mean_mu) - tf.exp(log_var), axis=1)\n","            self.add_loss(kl_loss)  # Ajoute la perte KL au modèle\n","            return kl_loss\n","\n","    # Initialisation de la couche de perte KL\n","    kl_loss_layer = KLLossLayer(name='kl_loss')([mean_mu, log_var])\n","\n","    # Construction du modèle encodeur final du VAE\n","    vae_encoder = Model(vae_encoder_input, [vae_encoder_output,kl_loss_layer], name='vae_encoder')\n","\n","    return vae_encoder_input, vae_encoder_output, mean_mu, log_var, shape_before_flattening, vae_encoder\n","\n","\n","vae_encoder_input, vae_encoder_output, mean_mu, log_var, shape_before_flattening, vae_encoder = build_vae_encoder(INPUT_DIM, Z_DIM)\n","vae_encoder.summary()"],"metadata":{"id":"nHuVeETi7WFQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# DECODEUR\n","\n","def build_vae_decoder(input_dim, shape_before_flattening):\n","    \"\"\"\n","    Construit le décodeur d'un autoencodeur variationnel (VAE) qui transforme un vecteur latent en une image reconstruite.\n","\n","    Paramètres:\n","    -----------\n","    input_dim : int\n","        La dimension du vecteur latent, qui est l'entrée du décodeur.\n","    shape_before_flattening : tuple\n","        La forme des données avant l'aplatissement dans l'encodeur, utilisée ici pour reconstruire les dimensions\n","        nécessaires lors de la génération de l'image.\n","\n","    Retourne :\n","    --------\n","    decoder_input : tensorflow.keras.layers.Input\n","        L'entrée du décodeur, prenant un vecteur de dimension `input_dim`.\n","    decoder_output : tensorflow.keras.layers.Layer\n","        La sortie finale du décodeur, une image reconstruite de forme (hauteur, largeur, canaux).\n","    decoder_model : tensorflow.keras.models.Model\n","        Le modèle Keras du décodeur, reliant `decoder_input` à `decoder_output`.\n","    \"\"\"\n","\n","    # Définir l'entrée du modèle de décodeur, qui est un vecteur latent de dimension `input_dim`\n","    decoder_input = Input(shape=(input_dim,), name='decoder_input')\n","\n","    # Conversion du vecteur en une matrice, en utilisant la forme avant Flatten (shape_before_flattening)\n","    x = Dense(np.prod(shape_before_flattening))(decoder_input)\n","    x = Reshape(shape_before_flattening)(x)\n","\n","    # Première couche Conv2DTranspose pour doubler la dimension spatiale et réduire le nombre de filtres\n","    x = Conv2DTranspose(filters=64,\n","                        kernel_size=3,\n","                        strides=2,\n","                        padding='same',\n","                        name='decoder_conv_1')(x)\n","    x = LeakyReLU()(x)  # Activation pour la reconstruction des détails\n","\n","    # Deuxième couche Conv2DTranspose pour continuer la reconstruction\n","    x = Conv2DTranspose(filters=64,\n","                        kernel_size=3,\n","                        strides=2,\n","                        padding='same',\n","                        name='decoder_conv_2')(x)\n","    x = LeakyReLU()(x)\n","\n","    # Troisième couche Conv2DTranspose pour doubler encore la dimension spatiale\n","    x = Conv2DTranspose(filters=32,\n","                        kernel_size=3,\n","                        strides=2,\n","                        padding='same',\n","                        name='decoder_conv_3')(x)\n","    x = LeakyReLU()(x)\n","\n","    # Quatrième et dernière couche Conv2DTranspose pour ramener les dimensions aux valeurs d'image\n","    x = Conv2DTranspose(filters=3,\n","                        kernel_size=3,\n","                        strides=2,\n","                        padding='same',\n","                        name='decoder_conv_4')(x)\n","    x = Activation('sigmoid')(x)  # Activation sigmoïde pour des valeurs de pixel entre 0 et 1\n","\n","    # La sortie du décodeur est l'image reconstruite\n","    decoder_output = x\n","\n","    # Retourne l'entrée, la sortie, et le modèle complet du décodeur\n","    return decoder_input, decoder_output, Model(decoder_input, decoder_output)\n","\n","\n","# Création du décodeur en utilisant la dimension de l'espace latent et la forme avant le flattening\n","vae_decoder_input, vae_decoder_output, vae_decoder = build_vae_decoder(input_dim = Z_DIM,\n","                                        shape_before_flattening = shape_before_flattening\n","                                        )\n","vae_decoder.summary()"],"metadata":{"id":"hL6__r9A7ZPD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Entrée principale du VAE (identique à l'entrée de l'encodeur)\n","vae_autoencoder_input = vae_encoder_input\n","\n","# Sortie du VAE en passant par le décodeur, en prenant la sortie de l'encodeur comme entrée\n","vae_autoencoder_output = vae_decoder(vae_encoder_output)\n","\n","# Création du modèle complet du VAE avec l'entrée et la sortie spécifiées\n","vae_autoencoder = Model(vae_autoencoder_input, vae_autoencoder_output)\n","\n","vae_autoencoder.summary()"],"metadata":{"id":"9rfZMpx87eGN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["B = 0.0001  # facteur de pondération pour la KL divergence\n","\n","def r_loss(y_true, y_pred):\n","    return K.mean(K.square(y_true - y_pred), axis=[1, 2, 3])  # erreur MSE\n","\n","def total_loss(y_true, y_pred):\n","    return K.mean(K.square(y_true - y_pred), axis=[1, 2, 3])  # juste la reconstruction\n","\n"],"metadata":{"id":"vmYOG8rT7gCh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["DATA_FOLDER = \"Data_Project/Tiger-Fox-Elephant/fox\"\n","NUM_IMAGES = len(os.listdir(DATA_FOLDER))  # Nombre d'images total dans le dossier\n","BATCH_SIZE = 256  # Taille des batchs\n","IMG_SIZE = (128, 128)  # Taille des images\n"],"metadata":{"id":"izm6xOGA7iDq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Callback pour sauvegarder les meilleurs poids\n","checkpoint_callback = ModelCheckpoint(\n","    filepath=\"vaeautoencoder_best_weights.weights.h5\",  # Nom du fichier pour sauvegarder les meilleurs poids\n","    save_best_only=True,                        # Sauvegarde uniquement si la perte de validation s'améliore\n","    monitor='val_loss',                         # Critère de suivi pour la validation\n","    save_weights_only=True,                     # Sauvegarde des poids\n","    mode='min',                                 # Mode de suivi (minimiser la perte)\n","    verbose=1                                   # Affiche un message lors de la sauvegarde\n",")\n","\n","# Callback pour arrêter l'entraînement si la perte de validation ne s'améliore pas\n","early_stopping_callback = EarlyStopping(\n","    monitor='val_loss',                         # Critère de suivi pour l'arrêt anticipé\n","    patience=30,                                # Arrête l'entraînement après 10 époques sans amélioration\n","    mode='min',                                 # Mode de suivi (minimiser la perte)\n","    restore_best_weights=True,                  # Restaure les meilleurs poids après l'arrêt\n","    verbose=1                                   # Affiche un message lors de l'arrêt\n",")"],"metadata":{"id":"tAq40-1T90PV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["LEARNING_RATE = 1e-4\n","EPOCHS = 400\n","# Définir la perte de reconstruction standard\n","# Compilation du modèle en intégrant total_loss comme perte\n","vae_autoencoder.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE), loss=total_loss)\n","\n","\n","# Calcule le nombre total d'images dans le répertoire spécifié\n","total_images = len(data_flow.filenames)\n","\n","# Définir `steps_per_epoch` en divisant le total d'images par le batch_size, en arrondissant vers le haut\n","steps_per_epoch = int(np.ceil(total_images / BATCH_SIZE))\n","\n","print(f\"Nombre total d'images : {total_images}\")\n","print(f\"Taille du batch : {BATCH_SIZE}\")\n","print(f\"Nombre de steps par epoch (steps_per_epoch) : {steps_per_epoch}\")\n","\n","# Entraînement du modèle avec les callbacks configurés\n","history = vae_autoencoder.fit(\n","    data_flow,\n","    shuffle=True,\n","    epochs=EPOCHS,\n","    steps_per_epoch=steps_per_epoch,\n","    validation_data=data_flow,\n","    callbacks=[checkpoint_callback, early_stopping_callback]  # Ajout des deux callbacks\n",")"],"metadata":{"id":"ZfQnyFef94uT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["show_images(vae_autoencoder,example_images)"],"metadata":{"id":"8_fks3bOD2dx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["visualize_latent_space(encoder_model=vae_autoencoder, images=example_images, num_images=5, image_size=(28, 28))"],"metadata":{"id":"5Ig3b5VF1w6D"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def generate_images_from_noise(n_to_show=10):\n","    \"\"\"\n","    Génère et affiche des images à partir d'un bruit aléatoire en utilisant le décodeur du VAE.\n","\n","    Cette fonction génère des échantillons à partir d'une distribution normale standard (N(0,1)),\n","    puis utilise le décodeur du Variational Autoencoder (VAE) pour transformer ces échantillons\n","    en images reconstruites. Les images générées sont ensuite affichées.\n","\n","    Paramètres :\n","    n_to_show (int, optionnel): Le nombre d'images à générer et afficher. Par défaut à 10.\n","\n","    \"\"\"\n","    # Génère des échantillons à partir d'une distribution normale (N(0,1)) pour les images à créer\n","    normal_distribution_sample = np.random.normal(0, 1, size=(n_to_show, Z_DIM))\n","\n","\n","\n","\n","    # Applique le décodeur pour transformer les échantillons en images\n","    reconst_images = vae_decoder.predict(normal_distribution_sample)\n","\n","    # Crée une figure pour afficher les images générées\n","    fig = plt.figure(figsize=(15, 3))\n","    fig.subplots_adjust(hspace=0.4, wspace=0.4)\n","\n","    # Affiche chaque image générée dans un sous-graphique\n","    for i in range(n_to_show):\n","        img = reconst_images[i].squeeze()  # Enlève les dimensions inutiles\n","        sub = fig.add_subplot(2, n_to_show, i + 1)  # Crée un sous-graphique pour chaque image\n","        sub.axis('off')  # Désactive les axes autour des images\n","        sub.imshow(img)  # Affiche l'image"],"metadata":{"id":"uqdXU0AS1xQ1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["generate_images_from_noise()"],"metadata":{"id":"mwuvIwEHyzud"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def generate_images_from_noise(n_to_show=10):\n","    \"\"\"\n","    Génère et affiche des images à partir de bruit aléatoire en utilisant le décodeur du VAE.\n","\n","    Cette fonction génère des échantillons à partir d'une distribution normale standard (N(0,1)),\n","    puis utilise le décodeur du Variational Autoencoder (VAE) pour transformer ces échantillons\n","    en images reconstruites. Les images générées sont ensuite affichées dans une figure avec un\n","    agencement flexible selon le nombre d'images à afficher.\n","\n","    Paramètres :\n","    n_to_show (int, optionnel): Le nombre d'images à générer et afficher. Par défaut à 10.\n","\n","    \"\"\"\n","    # Génère des échantillons à partir d'une distribution normale (N(0,1)) pour les images à créer\n","    normal_distribution_sample = np.random.normal(0, 1, size=(n_to_show, Z_DIM))\n","\n","    # Applique le décodeur pour transformer les échantillons en images\n","    reconst_images = vae_decoder.predict(normal_distribution_sample)\n","\n","    # Crée une figure pour afficher les images générées\n","    fig = plt.figure(figsize=(15, 15))  # Augmente la taille de la figure\n","    rows = int(np.ceil(n_to_show / 4))  # Calcule le nombre de lignes nécessaires\n","    columns = 4  # Définit le nombre de colonnes par ligne\n","    fig.subplots_adjust(hspace=0.4, wspace=0.4)  # Ajuste l'espacement entre les images\n","\n","    # Affiche chaque image générée dans un sous-graphique\n","    for i in range(n_to_show):\n","        img = reconst_images[i].squeeze()  # Enlève les dimensions inutiles\n","        sub = fig.add_subplot(rows, columns, i + 1)  # Positionne l'image dans la grille\n","        sub.axis('off')  # Désactive les axes autour des images\n","        sub.imshow(img)  # Affiche l'image\n","\n","\n","generate_images_from_noise()"],"metadata":{"id":"vxx0GNHS61Td"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}